{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "https://www.kaggle.com/code/cdeotte/catboost-starter-lb-0-60?scriptVersionId=158772898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREPARATION_VOTE_METHOD = \"sum_and_normalize\" # \"max_vote_window\" or \"sum_and_normalize\". Decides how to aggregate the predictions of the overlapping windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "import pathlib\n",
    "\n",
    "if bool(os.environ.get(\"KAGGLE_URL_BASE\", \"\")):\n",
    "  import sys\n",
    "  # running on kaggle\n",
    "  sys.path.insert(0, \"/kaggle/input/hsm-source-files\")\n",
    "else:\n",
    "  # running locally\n",
    "  import notebook_setup\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.utils.utils import get_raw_data_dir, get_processed_data_dir, get_submission_csv_path, running_in_kaggle\n",
    "from src.utils.constants import Constants\n",
    "from src.datasets.eeg_processor import EEGDataProcessor\n",
    "from src.utils.k_folds_creator import KFoldCreator\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor initialized.\n",
      "Raw data path: '/home/david/git/aicomp/data'\n",
      "Processed data path: '/home/david/git/aicomp/data/processed'\n",
      "==================================================\n",
      "Starting EEG Data Processing Pipeline\n",
      "==================================================\n",
      "Skipping NumPy file creation as requested.\n",
      "Using 'sum_and_normalize' vote aggregation strategy with spectrogram info.\n",
      "\n",
      "Processed train data saved to '/home/david/git/aicomp/data/processed/train_processed.csv'.\n",
      "Shape of the final dataframe: (17089, 12)\n",
      "\n",
      "Pipeline finished successfully!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = get_raw_data_dir()\n",
    "\n",
    "processor = EEGDataProcessor(raw_data_path=DATA_PATH, processed_data_path=get_processed_data_dir())\n",
    "train_df = processor.process_data(vote_method=DATA_PREPARATION_VOTE_METHOD, skip_npy=True)\n",
    "\n",
    "test_df = pd.read_csv(DATA_PATH / \"test.csv\")\n",
    "\n",
    "kl_score = nn.KLDivLoss(reduction=\"batchmean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We need features for the XGBoost model.\n",
    "For this, we take the mean over time for all of the 400 spectrogram frequencies.\n",
    "We take the middle 10 minutes of all spectrograms.\n",
    "For each EEG ID, this produces 400 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spectrogram Files into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11138 spectrogram files to load into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11138/11138 [06:14<00:00, 29.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all spectrograms into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spectrograms_dir = DATA_PATH / \"train_spectrograms\"\n",
    "spectrogram_files = list(spectrograms_dir.glob(\"*.parquet\"))\n",
    "print(f\"Found {len(spectrogram_files)} spectrogram files to load into memory\")\n",
    "\n",
    "def get_spectrogram_content(spectromgram_file: pathlib.Path):\n",
    "  spectrogram_id = int(file.stem.split(\"_\")[-1])\n",
    "  content = pd.read_parquet(file)\n",
    "  content = content.drop(columns=[\"time\"]).values\n",
    "  return spectrogram_id, content\n",
    "\n",
    "spectrograms = {}\n",
    "for file in tqdm(spectrogram_files):\n",
    "  spectrogram_id, content = get_spectrogram_content(file)\n",
    "  spectrograms[spectrogram_id] = content\n",
    "\n",
    "gc.collect()\n",
    "print(\"Loaded all spectrograms into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17089 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spectrograms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_df)), total=\u001b[38;5;28mlen\u001b[39m(train_df)):\n\u001b[32m     12\u001b[39m   row = train_df.iloc[i]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m   data[i,:] = \u001b[43mextract_spectrogram_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mextract_spectrogram_features\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m      6\u001b[39m middle_offset = (row[\u001b[33m'\u001b[39m\u001b[33mmin_offset\u001b[39m\u001b[33m'\u001b[39m] + row[\u001b[33m'\u001b[39m\u001b[33mmax_offset\u001b[39m\u001b[33m'\u001b[39m]) // \u001b[32m2\u001b[39m \u001b[38;5;66;03m# this the middle between the least spectrogram offset and greatest spectogram offset\u001b[39;00m\n\u001b[32m      7\u001b[39m row_index = \u001b[38;5;28mint\u001b[39m(middle_offset // \u001b[32m2\u001b[39m) \u001b[38;5;66;03m# each spectrogram row corresponds to 2s, so we divide by 2 to get the row index\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m average_frequencies = np.array(\u001b[43mspectrograms\u001b[49m[spectrogram_id][row_index:row_index+\u001b[32m300\u001b[39m,:] ).mean(axis=\u001b[32m0\u001b[39m) \u001b[38;5;66;03m# average over 300 rows (10 minutes)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m average_frequencies\n",
      "\u001b[31mNameError\u001b[39m: name 'spectrograms' is not defined"
     ]
    }
   ],
   "source": [
    "FEATURES = [f'freq_{x}' for x in range(400)]\n",
    "data = np.zeros((len(train_df),len(FEATURES)))\n",
    "\n",
    "def extract_spectrogram_features(row):\n",
    "  spectrogram_id = int(row[\"spectrogram_id\"])\n",
    "  middle_offset = (row['min_offset'] + row['max_offset']) // 2 # this the middle between the least spectrogram offset and greatest spectogram offset\n",
    "  row_index = int(middle_offset // 2) # each spectrogram row corresponds to 2s, so we divide by 2 to get the row index\n",
    "  average_frequencies = np.array(spectrograms[spectrogram_id][row_index:row_index+300,:] ).mean(axis=0) # average over 300 rows (10 minutes)\n",
    "  return average_frequencies\n",
    "\n",
    "for i in tqdm(range(len(train_df)), total=len(train_df)):\n",
    "  row = train_df.iloc[i]\n",
    "  data[i,:] = extract_spectrogram_features(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m, category=pd.errors.PerformanceWarning)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_df[FEATURES] = \u001b[43mdata\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m data\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m spectrograms\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "train_df[FEATURES] = data\n",
    "\n",
    "del data\n",
    "del spectrograms\n",
    "gc.collect()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_creator = KFoldCreator(n_splits=N_SPLITS, seed=Constants.SEED)\n",
    "train_folds_df = fold_creator.create_folds(\n",
    "    df=train_df, stratify_col=\"expert_consensus\", group_col=\"patient_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "FOLD 0\n",
      "Train size: 13755, Valid size: 3334\n",
      "==============================\n",
      "[0]\teval-mlogloss:1.63018\n",
      "[26]\teval-mlogloss:1.35269\n",
      "========================================\n",
      "FOLD 1\n",
      "Train size: 13151, Valid size: 3938\n",
      "==============================\n",
      "[0]\teval-mlogloss:1.60886\n",
      "[27]\teval-mlogloss:1.32410\n",
      "========================================\n",
      "FOLD 2\n",
      "Train size: 13422, Valid size: 3667\n",
      "==============================\n",
      "[0]\teval-mlogloss:1.62823\n",
      "[22]\teval-mlogloss:1.32224\n",
      "========================================\n",
      "FOLD 3\n",
      "Train size: 14356, Valid size: 2733\n",
      "==============================\n",
      "[0]\teval-mlogloss:1.61536\n",
      "[31]\teval-mlogloss:1.23553\n",
      "========================================\n",
      "FOLD 4\n",
      "Train size: 13672, Valid size: 3417\n",
      "==============================\n",
      "[0]\teval-mlogloss:1.64184\n",
      "[20]\teval-mlogloss:1.42018\n"
     ]
    }
   ],
   "source": [
    "# warnings.filterwarnings(\"ignore\", message=\"Falling back to prediction using DMatrix due to mismatched devices\")\n",
    "\n",
    "all_oof = []\n",
    "all_true = []\n",
    "TARS = {'Seizure':0, 'LPD':1, 'GPD':2, 'LRDA':3, 'GRDA':4, 'Other':5}\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_train_df = train_folds_df[train_folds_df[\"fold\"] != fold].reset_index(drop=True)\n",
    "    fold_valid_df = train_folds_df[train_folds_df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"FOLD {fold}\")\n",
    "    print(f\"Train size: {len(fold_train_df)}, Valid size: {len(fold_valid_df)}\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    X_train = fold_train_df[FEATURES]\n",
    "    y_train = fold_train_df[\"expert_consensus\"].map(TARS)\n",
    "    \n",
    "    X_valid = fold_valid_df[FEATURES]\n",
    "    y_valid = fold_valid_df[\"expert_consensus\"].map(TARS)\n",
    "\n",
    "    # Create DMatrix objects\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    # Set parameters\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": len(Constants.TARGETS),\n",
    "        \"device\": \"cuda\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"seed\": Constants.SEED,\n",
    "    }\n",
    "\n",
    "    # Train model\n",
    "    evals = [(dvalid, \"eval\")]\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=300,\n",
    "        evals=evals,\n",
    "        verbose_eval=100,\n",
    "        early_stopping_rounds=10,\n",
    "    )\n",
    "    \n",
    "    model.save_model(f\"XGB_fold_{fold}.json\")\n",
    "\n",
    "    # Predict probabilities\n",
    "    oof = model.predict(dvalid)\n",
    "    all_oof.extend(oof)\n",
    "\n",
    "    all_true.extend(fold_valid_df[Constants.TARGETS].values)\n",
    "\n",
    "    del X_train, y_train, X_valid, y_valid, dtrain, dvalid, oof\n",
    "    gc.collect()\n",
    "\n",
    "all_oof = np.array(all_oof)\n",
    "all_true = np.array(all_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF KL Score: 1.055870532989502\n"
     ]
    }
   ],
   "source": [
    "all_oof_tensor = torch.tensor(all_oof, dtype=torch.float32)\n",
    "all_true_tensor = torch.tensor(all_true, dtype=torch.float32)\n",
    "\n",
    "kl_score = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "score = kl_score(all_oof_tensor.log(), all_true_tensor).item()\n",
    "\n",
    "print(f\"OOF KL Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer on Test and create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
