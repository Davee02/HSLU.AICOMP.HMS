{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "706140a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREPARATION_VOTE_METHOD = \"max_vote_window\" # \"max_vote_window\" or \"sum_and_normalize\". Decides how to aggregate the predictions of the overlapping windows\n",
    "EXISTING_CHECKPOINT_KAGGLE_DATASET_ID = \"hsm-models\" # set to None if you want to train a new model on Kaggle. Else, set to the Kaggle dataset ID where the existing model checkpoints are stored\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"tf_efficientnet_b0_ns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7afc1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "if bool(os.environ.get(\"KAGGLE_URL_BASE\", \"\")):\n",
    "  import sys\n",
    "  # running on kaggle\n",
    "  sys.path.insert(0, \"/kaggle/input/hsm-source-files\")\n",
    "else:\n",
    "  # running locally\n",
    "  sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "\n",
    "from src.utils.utils import get_raw_data_dir, get_submission_csv_path, get_models_save_path, set_seeds\n",
    "from src.utils.constants import Constants\n",
    "\n",
    "from src.datasets.spectrogram_dataset import SpectrogramDataset\n",
    "from src.models.base_cnn import BaseCNN\n",
    "\n",
    "set_seeds(Constants.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61b3c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = get_raw_data_dir()\n",
    "test_df = pd.read_csv(DATA_PATH / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a3cd5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>853520</td>\n",
       "      <td>3911565283</td>\n",
       "      <td>6885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spectrogram_id      eeg_id  patient_id\n",
       "0          853520  3911565283        6885"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf1d8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfCFG:\n",
    "    data_path = DATA_PATH\n",
    "    model_name = PRETRAINED_MODEL_NAME_OR_PATH\n",
    "    target_size = 6\n",
    "    img_size = (128, 256)\n",
    "    n_splits = 5\n",
    "    batch_size = 64\n",
    "    num_workers = 8\n",
    "    model_dir = get_models_save_path(EXISTING_CHECKPOINT_KAGGLE_DATASET_ID) / \"base_cnn\" / PRETRAINED_MODEL_NAME_OR_PATH / DATA_PREPARATION_VOTE_METHOD\n",
    "\n",
    "InfCFG.model_paths = [os.path.join(InfCFG.model_dir, f'best_model_fold{i}.pth') for i in range(InfCFG.n_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50c0480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    test_df = pd.read_csv(os.path.join(InfCFG.data_path, 'test.csv'))\n",
    "    test_dataset = SpectrogramDataset(\n",
    "        df=test_df, \n",
    "        targets=Constants.TARGETS, \n",
    "        data_path=InfCFG.data_path, \n",
    "        img_size=InfCFG.img_size, \n",
    "        mode='test'\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=InfCFG.batch_size, shuffle=False, num_workers=InfCFG.num_workers\n",
    "    )\n",
    "\n",
    "    all_fold_predictions = []\n",
    "\n",
    "    for i, path in enumerate(InfCFG.model_paths):\n",
    "        print(f\"\\n========== Inferencing with Fold {i} Model ==========\")\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Model file not found: {path}. Skipping this fold.\")\n",
    "            continue\n",
    "            \n",
    "        model = BaseCNN(InfCFG.model_name, pretrained=False, num_classes=InfCFG.target_size)\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        current_fold_preds = []\n",
    "        with torch.no_grad():\n",
    "            for images in tqdm(test_loader, desc=f\"Predicting Fold {i}\"):\n",
    "                outputs = model(images.to(device))\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                current_fold_preds.append(probs)\n",
    "        \n",
    "        all_fold_predictions.append(np.concatenate(current_fold_preds))\n",
    "\n",
    "    if not all_fold_predictions:\n",
    "        print(\"No models were found for inference. Aborting.\")\n",
    "        return\n",
    "\n",
    "    avg_predictions = np.mean(all_fold_predictions, axis=0)\n",
    "    submission = pd.DataFrame({\"eeg_id\": test_df[\"eeg_id\"]})\n",
    "    submission[Constants.TARGETS] = avg_predictions\n",
    "\n",
    "    submission.to_csv(get_submission_csv_path(), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7a5fef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Inferencing with Fold 0 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/miniconda3/envs/aicomp/lib/python3.12/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n",
      "Predicting Fold 0: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 1 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 1: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 2 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 2: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 3 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 3: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 4 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 4: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n"
     ]
    }
   ],
   "source": [
    "run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
