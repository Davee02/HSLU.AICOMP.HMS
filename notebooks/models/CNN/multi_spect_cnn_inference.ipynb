{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706140a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREPARATION_VOTE_METHOD = \"max_vote_window\" # \"max_vote_window\" or \"sum_and_normalize\". Decides how to aggregate the predictions of the overlapping windows\n",
    "EXISTING_CHECKPOINT_KAGGLE_DATASET_ID = \"hsm-models\" # set to None if you want to train a new model on Kaggle. Else, set to the Kaggle dataset ID where the existing model checkpoints are stored\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"inception_v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afc1911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 09:23:20,385 :: root :: INFO :: Initialising Utils\n",
      "2025-11-14 09:23:22,224 :: root :: INFO :: Initialising Datasets\n",
      "2025-11-14 09:23:23,341 :: root :: INFO :: Initialising Models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping module tcn due to missing dependency: No module named 'pytorch_tcn'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/miniconda3/envs/aicomp/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/david/miniconda3/envs/aicomp/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "if bool(os.environ.get(\"KAGGLE_URL_BASE\", \"\")):\n",
    "  import sys\n",
    "  # running on kaggle\n",
    "  sys.path.insert(0, \"/kaggle/input/hsm-source-files\")\n",
    "else:\n",
    "  # running locally\n",
    "  sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "\n",
    "from src.utils.utils import get_raw_data_dir, get_processed_data_dir, get_submission_csv_path, get_models_save_path, set_seeds\n",
    "from src.utils.constants import Constants\n",
    "\n",
    "from src.datasets.multi_spectrogram import MultiSpectrogramDataset\n",
    "from src.models.base_cnn import BaseCNN\n",
    "from src.utils.eeg_spectrogram_creator import EEGSpectrogramGenerator\n",
    "\n",
    "set_seeds(Constants.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b3c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = get_raw_data_dir()\n",
    "test_df = pd.read_csv(DATA_PATH / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a3cd5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>853520</td>\n",
       "      <td>3911565283</td>\n",
       "      <td>6885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spectrogram_id      eeg_id  patient_id\n",
       "0          853520  3911565283        6885"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf1d8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfCFG:\n",
    "    data_path = DATA_PATH\n",
    "    model_name = PRETRAINED_MODEL_NAME_OR_PATH\n",
    "    target_size = 6\n",
    "    img_size = (128, 256)\n",
    "    n_splits = 5\n",
    "    batch_size = 64\n",
    "    num_workers = 8\n",
    "    model_dir = get_models_save_path(EXISTING_CHECKPOINT_KAGGLE_DATASET_ID) / \"multi_spec_cnn\" / PRETRAINED_MODEL_NAME_OR_PATH / DATA_PREPARATION_VOTE_METHOD\n",
    "    test_eeg_spec_path = get_processed_data_dir() / \"eeg_spectrograms\" / \"test\" / \"cwt\"\n",
    "    dropout_p = 0.1\n",
    "\n",
    "InfCFG.model_paths = [os.path.join(InfCFG.model_dir, f'best_model_fold{i}.pth') for i in range(InfCFG.n_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe663b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test EEG spectrogram path: /home/david/git/aicomp/data/processed/eeg_spectrograms/test/cwt\n",
      "Test EEG spectrograms already exist. Skipping generation.\n"
     ]
    }
   ],
   "source": [
    "InfCFG.test_eeg_spec_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Test EEG spectrogram path: {InfCFG.test_eeg_spec_path}\")\n",
    "\n",
    "existing_specs = len(list(InfCFG.test_eeg_spec_path.glob(\"*.npy\")))\n",
    "\n",
    "test_eeg_ids = test_df[\"eeg_id\"].unique()\n",
    "\n",
    "if existing_specs == len(test_eeg_ids):\n",
    "  print(\"Test EEG spectrograms already exist. Skipping generation.\")\n",
    "else:\n",
    "  spectrogram_creator = EEGSpectrogramGenerator([\"cwt\"])\n",
    "  for eeg_id in tqdm(test_eeg_ids, desc=\"Generating EEG Spectrograms\"):\n",
    "      eeg_path = os.path.join(InfCFG.data_path, \"test_eegs\", f\"{eeg_id}.parquet\")\n",
    "      eeg = pd.read_parquet(eeg_path)\n",
    "      spectrograms = spectrogram_creator.generate(eeg)\n",
    "      np.save(InfCFG.test_eeg_spec_path / f\"{eeg_id}.npy\", spectrograms['cwt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c0480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    test_df = pd.read_csv(os.path.join(InfCFG.data_path, 'test.csv'))\n",
    "    test_dataset = MultiSpectrogramDataset(\n",
    "        df=test_df, \n",
    "        targets=Constants.TARGETS, \n",
    "        data_path=InfCFG.data_path, \n",
    "        img_size=InfCFG.img_size, \n",
    "        eeg_spec_path=InfCFG.test_eeg_spec_path, \n",
    "        mode='test',\n",
    "        apply_augmentations=False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=InfCFG.batch_size, shuffle=False, num_workers=InfCFG.num_workers\n",
    "    )\n",
    "\n",
    "    all_fold_predictions = []\n",
    "\n",
    "    for i, path in enumerate(InfCFG.model_paths):\n",
    "        print(f\"\\n========== Inferencing with Fold {i} Model ==========\")\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Model file not found: {path}. Skipping this fold.\")\n",
    "            continue\n",
    "            \n",
    "        model = BaseCNN(InfCFG.model_name, pretrained=False, num_classes=InfCFG.target_size, dropout_p=InfCFG.dropout_p)\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        current_fold_preds = []\n",
    "        with torch.no_grad():\n",
    "            for images in tqdm(test_loader, desc=f\"Predicting Fold {i}\"):\n",
    "                outputs = model(images.to(device))\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                current_fold_preds.append(probs)\n",
    "        \n",
    "        all_fold_predictions.append(np.concatenate(current_fold_preds))\n",
    "\n",
    "    if not all_fold_predictions:\n",
    "        print(\"No models were found for inference. Aborting.\")\n",
    "        return\n",
    "\n",
    "    avg_predictions = np.mean(all_fold_predictions, axis=0)\n",
    "    submission = pd.DataFrame({\"eeg_id\": test_df[\"eeg_id\"]})\n",
    "    submission[Constants.TARGETS] = avg_predictions\n",
    "\n",
    "    submission.to_csv(get_submission_csv_path(), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a5fef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Inferencing with Fold 0 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 0: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 1 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 1: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 2 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 2: 100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 3 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 3: 100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 4 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 4: 100%|██████████| 1/1 [00:00<00:00,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
