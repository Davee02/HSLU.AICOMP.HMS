{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144581f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 16:27:56,661 :: root :: INFO :: Initialising Utils\n",
      "2025-10-10 16:27:57,143 :: root :: INFO :: Initialising Datasets\n",
      "2025-10-10 16:27:57,147 :: root :: INFO :: Initialising Models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_PREPARATION_VOTE_METHOD = \"sum_and_normalize\" \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "\n",
    "if bool(os.environ.get(\"KAGGLE_URL_BASE\", \"\")):\n",
    "    sys.path.insert(0, \"/kaggle/input/hsm-source-files\")\n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "\n",
    "from src.utils.utils import get_models_save_path, get_submission_csv_path, running_in_kaggle, get_raw_data_dir\n",
    "from src.utils.constants import Constants\n",
    "from src.datasets.eeg_dataset import EEGDataset\n",
    "from src.models.gru import GRUModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0242e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1598e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = get_raw_data_dir()\n",
    "class InfCFG:\n",
    "    model_name = 'GRU'\n",
    "    hidden_units = 128\n",
    "    num_layers = 2\n",
    "    target_size = 6\n",
    "    num_channels = 20\n",
    "    \n",
    "    data_path = DATA_PATH\n",
    "    n_splits = 5\n",
    "    \n",
    "    batch_size = 32\n",
    "    num_workers = 0\n",
    "    \n",
    "    if running_in_kaggle():\n",
    "        MODEL_DIR = f'/kaggle/input/gru-sum-votes/'\n",
    "    else:\n",
    "        MODEL_DIR = get_models_save_path() / \"GRUModel\" / DATA_PREPARATION_VOTE_METHOD\n",
    "\n",
    "InfCFG.model_paths = [os.path.join(InfCFG.MODEL_DIR, f'best_model_fold{i}.pth') for i in range(InfCFG.n_splits)]\n",
    "\n",
    "TARGETS = Constants.TARGETS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab5a28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/maiko/Documents/HSLU/AICOMP/HSLU.AICOMP.HMS/data')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e62f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Inferencing with Fold 0 Model ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b446d3215a45d0b5bd33dcb36e4587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 1 Model ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5518075d03c94308a647c53b8c7730e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 2 Model ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e48767ff8a047e394375179ad0ed363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 3 Model ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1d1d38154d4082b2b2b657d6bb096e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 3:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 4 Model ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e1f9d7a28a4082a0cd4f06b120a7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 4:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       eeg_id  seizure_vote  lpd_vote  gpd_vote  lrda_vote  grda_vote  \\\n",
      "0  3911565283      0.072843  0.082108  0.027828   0.028282   0.030526   \n",
      "\n",
      "   other_vote  \n",
      "0    0.758413  \n"
     ]
    }
   ],
   "source": [
    "def run_inference():\n",
    "    \"\"\"Executes the main inference loop.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    test_df = pd.read_csv(os.path.join(InfCFG.data_path, 'test.csv'))\n",
    "    \n",
    "    test_dataset = EEGDataset(df=test_df, data_path=InfCFG.data_path, mode='test')\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=InfCFG.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=InfCFG.num_workers\n",
    "    )\n",
    "\n",
    "    all_fold_predictions = []\n",
    "\n",
    "    for i, path in enumerate(InfCFG.model_paths):\n",
    "        print(f\"\\n========== Inferencing with Fold {i} Model ==========\")\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Model file not found: {path}. Skipping this fold.\")\n",
    "            continue\n",
    "            \n",
    "        model = GRUModel(\n",
    "            input_size=InfCFG.num_channels,\n",
    "            hidden_size=InfCFG.hidden_units,\n",
    "            num_layers=InfCFG.num_layers,\n",
    "            num_classes=InfCFG.target_size\n",
    "        )\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        current_fold_preds = []\n",
    "        with torch.no_grad():\n",
    "            for signals in tqdm(test_loader, desc=f\"Predicting Fold {i}\"):\n",
    "                outputs = model(signals.to(device))\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                current_fold_preds.append(probs)\n",
    "        \n",
    "        all_fold_predictions.append(np.concatenate(current_fold_preds))\n",
    "\n",
    "    if not all_fold_predictions:\n",
    "        print(\"No models were found for inference. Aborting.\")\n",
    "        return\n",
    "\n",
    "    avg_predictions = np.mean(all_fold_predictions, axis=0)\n",
    "    \n",
    "    submission = pd.DataFrame({\"eeg_id\": test_df[\"eeg_id\"]})\n",
    "    submission[TARGETS] = avg_predictions\n",
    "    submission.to_csv(get_submission_csv_path(), index=False)\n",
    "    \n",
    "\n",
    "    print(submission.head())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ab38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
