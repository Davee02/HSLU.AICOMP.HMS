{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5df102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 09:52:56,885 :: root :: INFO :: Initialising Utils\n",
      "2025-11-28 09:52:57,330 :: root :: INFO :: Initialising Datasets\n",
      "2025-11-28 09:52:57,342 :: root :: INFO :: Initialising Models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaikotrede\u001b[0m (\u001b[33mhms-hslu-aicomp-hs25\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm \n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "from src.utils.k_folds_creator import KFoldCreator\n",
    "from src.utils.utils import get_models_save_path\n",
    "from src.utils.constants import Constants \n",
    "from src.datasets.eeg_dataset_montage import EEGDatasetMontage\n",
    "from src.models.gru_convolution_attention import NodeAttentionModel\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54816519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    data_path = '../../../data/'\n",
    "\n",
    "    model_name = 'GRUConvNodeAttentionModel'\n",
    "    hidden_units = 256\n",
    "    num_layers = 1\n",
    "    target_size = 6 \n",
    "    \n",
    "    num_cnn_blocks = 4 \n",
    "    \n",
    "    sampling_rate = 200 # Hz\n",
    "    sequence_duration = 50 \n",
    "    downsample_factor = 3\n",
    "    \n",
    "    num_channels = 19\n",
    "    \n",
    "    dropout = 0.4\n",
    "    batch_size = 32\n",
    "    num_workers = 8\n",
    "    epochs = 50\n",
    "    lr = 1e-4\n",
    "    patience = 10\n",
    "    min_delta = 0.001\n",
    "    \n",
    "    use_attention = True\n",
    "\n",
    "    use_mixup = False      \n",
    "    mixup_alpha = 1.0      # beta distribution parameter (1.0 = uniform distribution between 0 and 1)\n",
    "    \n",
    "    \n",
    "\n",
    "CFG.sequence_length = CFG.sequence_duration * CFG.sampling_rate \n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "TARGETS = Constants.TARGETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ac0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, device='cuda'):\n",
    "    \"\"\"\n",
    "    Applies MixUp to inputs and targets.\n",
    "    Returns mixed inputs and mixed targets.\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "\n",
    "    mixed_y = lam * y + (1 - lam) * y[index, :]\n",
    "    \n",
    "    return mixed_x, mixed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fe4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(df, fold_id):\n",
    "    train_df = df[df['fold'] != fold_id].reset_index(drop=True)\n",
    "    valid_df = df[df['fold'] == fold_id].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = EEGDatasetMontage(df=train_df, data_path=CFG.data_path, mode='train', downsample_factor=CFG.downsample_factor)\n",
    "\n",
    "    valid_dataset = EEGDatasetMontage(df=valid_df, data_path=CFG.data_path, mode='valid', downsample_factor=CFG.downsample_factor)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=CFG.batch_size, shuffle=True,\n",
    "        num_workers=CFG.num_workers, pin_memory=True, drop_last=True, persistent_workers=True if CFG.num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=CFG.batch_size, shuffle=False,\n",
    "        num_workers=CFG.num_workers, pin_memory=True, drop_last=False, persistent_workers=True if CFG.num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb8515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data and creating folds...\n",
      "Train shape: (17089, 15)\n",
      "Targets ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
      "Folds created. Value counts per fold:\n",
      "fold\n",
      "0    4067\n",
      "1    3658\n",
      "2    3381\n",
      "4    3358\n",
      "3    2625\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def run_training(df, DATA_PREPARATION_VOTE_METHOD):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    autocast_enabled = (device.type == 'cuda')\n",
    "    \n",
    "    all_oof = []\n",
    "    all_true = []\n",
    "    \n",
    "    fold_scores = []\n",
    "\n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"\\n========== FOLD {fold} ==========\")\n",
    "\n",
    "        experiment_group = f\"montages_block_{CFG.num_cnn_blocks}_attention_{CFG.use_attention}_attentionpool4headsMP_meanmaxpoolclass)\"\n",
    "        experiment_name = f\"{experiment_group}_fold{fold}\"\n",
    "\n",
    "        config = {\n",
    "            \"architecture\": CFG.model_name, \"hidden_units\": CFG.hidden_units, \n",
    "            \"num_layers\": CFG.num_layers,\n",
    "            \"fold\": fold, \"features\": \"raw_eeg\", \n",
    "            \"sequence_duration\": f\"{CFG.sequence_duration}s\",\n",
    "            \"optimizer\": \"AdamW\", \"learning_rate\": CFG.lr, \n",
    "            \"batch_size\": CFG.batch_size,\n",
    "            \"epochs\": CFG.epochs, \"seed\": CFG.seed, \n",
    "            \"Scheduler\": \"CosineAnnealingLR\",\n",
    "            \n",
    "            \"num_cnn_blocks\": CFG.num_cnn_blocks \n",
    "        }\n",
    "\n",
    "        wandb.init(\n",
    "            project=\"hms-aicomp-gru-conv\",\n",
    "            name=experiment_name,  \n",
    "            group=experiment_group,\n",
    "            tags=['gru-conv', f'fold{fold}', f'montages_block_{CFG.num_cnn_blocks}_attention_{CFG.use_attention}'],\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        model = NodeAttentionModel(\n",
    "            num_nodes=CFG.num_channels,       \n",
    "            node_embed_size=256,              \n",
    "            hidden_size=CFG.hidden_units,    \n",
    "            num_layers=CFG.num_layers,       \n",
    "            num_classes=CFG.target_size,     \n",
    "            num_cnn_blocks=CFG.num_cnn_blocks,\n",
    "            dropout=CFG.dropout,\n",
    "            use_inception=True              \n",
    "        )\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=1e-2)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.epochs)\n",
    "        loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
    "        train_loader, valid_loader = get_dataloaders(df, fold)\n",
    "\n",
    "        scaler = GradScaler(enabled=autocast_enabled)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_path = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(CFG.epochs):\n",
    "            print(f\"   --- Epoch {epoch+1}/{CFG.epochs} ---\")\n",
    "            \n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for signals, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "                signals, labels = signals.to(device), labels.to(device)\n",
    "                if CFG.use_mixup:\n",
    "                    signals, labels = mixup_data(signals, labels, alpha=CFG.mixup_alpha, device=device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with autocast(enabled=autocast_enabled, device_type=device.type):\n",
    "                    outputs = model(signals)\n",
    "                    log_probs = F.log_softmax(outputs, dim=1)\n",
    "                    loss = loss_fn(log_probs, labels)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                scaler.unscale_(optimizer)\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                train_loss += loss.item() * signals.size(0)\n",
    "                wandb.log({\"train/loss\": loss.item()})\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for i, (signals, labels) in enumerate(tqdm(valid_loader, desc=\"Validation\")):\n",
    "                    signals, labels = signals.to(device), labels.to(device)\n",
    "                    \n",
    "                    with autocast(enabled=autocast_enabled, device_type=device.type):\n",
    "                        outputs = model(signals)\n",
    "                        log_probs = F.log_softmax(outputs, dim=1)\n",
    "                        loss = loss_fn(log_probs, labels)\n",
    "                        \n",
    "                    valid_loss += loss.item() * signals.size(0)\n",
    "\n",
    "            valid_loss /= len(valid_loader.dataset)\n",
    "            \n",
    "            epoch_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"   Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Valid Loss = {valid_loss:.4f}, LR = {epoch_lr:.6f}\")\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1, \"train/epoch_loss\": train_loss, \"val/loss\": valid_loss,\n",
    "                \"val/kl_div\": valid_loss, \"train/epoch_lr\": epoch_lr\n",
    "            })\n",
    "\n",
    "            if valid_loss < best_val_loss - CFG.min_delta:\n",
    "                best_val_loss = valid_loss\n",
    "                patience_counter = 0 \n",
    "                \n",
    "                best_model_path = get_models_save_path() / \"GRUConvNodeAttentionModel\" / DATA_PREPARATION_VOTE_METHOD / f'best_model_fold{fold}.pth'\n",
    "                best_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"   New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"   No improvement. Patience: {patience_counter}/{CFG.patience}\")\n",
    "\n",
    "            if patience_counter >= CFG.patience:\n",
    "                print(f\"   Early stopping triggered after {patience_counter} epochs without improvement.\")\n",
    "                wandb.log({\"early_stopped_epoch\": epoch + 1})\n",
    "                break \n",
    "            scheduler.step()\n",
    "        \n",
    "        fold_scores.append(best_val_loss)\n",
    "        wandb.summary['best_val_kl_div'] = best_val_loss\n",
    "\n",
    "        if best_model_path:\n",
    "            print(f\"Loading best model from fold {fold} to generate OOF predictions...\")\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "            model.eval()\n",
    "\n",
    "            fold_oof_preds = []\n",
    "            fold_oof_labels = []\n",
    "            with torch.no_grad():\n",
    "                for signals, labels in tqdm(valid_loader, desc=f\"Generating OOF for Fold {fold}\"):\n",
    "                    signals = signals.to(device)\n",
    "                    \n",
    "                    with autocast(enabled=autocast_enabled, device_type=device.type):\n",
    "                        outputs = model(signals)\n",
    "                        probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                        \n",
    "                    fold_oof_preds.append(probs)\n",
    "                    fold_oof_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            all_oof.extend(np.concatenate(fold_oof_preds))\n",
    "            all_true.extend(np.concatenate(fold_oof_labels))\n",
    "\n",
    "            artifact = wandb.Artifact(f'model-fold{fold}', type='model')\n",
    "            artifact.add_file(best_model_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "            print(f\"\\nLogged artifact for fold {fold} with best validation loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            print(\"\\nNo best model was saved during training for this fold.\")\n",
    "        \n",
    "        wandb.finish()\n",
    "\n",
    "    print(\"\\nCalculating final OOF CV score from all collected predictions...\")\n",
    "    \n",
    "    valid_indices = df[df['fold'].isin(range(CFG.n_splits))].index\n",
    "    oof_df = pd.DataFrame(all_oof, index=valid_indices, columns=TARGETS)\n",
    "    true_df = pd.DataFrame(all_true, index=valid_indices, columns=TARGETS)\n",
    "    \n",
    "    oof_df = oof_df.sort_index()\n",
    "    true_df = true_df.sort_index()\n",
    "\n",
    "    oof_tensor = torch.tensor(oof_df.values, dtype=torch.float32)\n",
    "    true_tensor = torch.tensor(true_df.values, dtype=torch.float32)\n",
    "\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    overall_cv_score = kl_loss(torch.log(oof_tensor), true_tensor).item()\n",
    "\n",
    "    return overall_cv_score, fold_scores\n",
    "\n",
    "DATA_PREPARATION_VOTE_METHOD = \"max_vote_window\"\n",
    "\n",
    "print(\"Preparing data and creating folds...\")\n",
    "df = pd.read_csv(CFG.data_path + 'processed_data_max_vote_window.csv') \n",
    "\n",
    "label_map = {t: i for i, t in enumerate(TARGETS)}\n",
    "df['expert_consensus'] = df[TARGETS].idxmax(axis=1)\n",
    "\n",
    "print('Train shape:', df.shape)\n",
    "print('Targets', list(TARGETS))\n",
    "\n",
    "fold_creator = KFoldCreator(n_splits=CFG.n_splits, seed=CFG.seed)\n",
    "df = fold_creator.create_folds(df, stratify_col='expert_consensus', group_col='patient_id')\n",
    "\n",
    "print(\"Folds created. Value counts per fold:\")\n",
    "print(df['fold'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c7d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting systematic evaluation for num_cnn_blocks: [3]...\n",
      "\n",
      "============================================================\n",
      "      STARTING EXPERIMENT: 3 CNN BLOCKS\n",
      "============================================================\n",
      "Using device: cuda\n",
      "\n",
      "========== FOLD 0 ==========\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/maiko/Documents/HSLU/AICOMP/HSLU.AICOMP.HMS/notebooks/models/GRU/wandb/run-20251128_095259-6qgvfyti</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hms-hslu-aicomp-hs25/hms-aicomp-gru-conv/runs/6qgvfyti' target=\"_blank\">montages_block_3_attention_True_attentionpool4headsMP_meanmaxpoolclass)_fold0</a></strong> to <a href='https://wandb.ai/hms-hslu-aicomp-hs25/hms-aicomp-gru-conv' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hms-hslu-aicomp-hs25/hms-aicomp-gru-conv' target=\"_blank\">https://wandb.ai/hms-hslu-aicomp-hs25/hms-aicomp-gru-conv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hms-hslu-aicomp-hs25/hms-aicomp-gru-conv/runs/6qgvfyti' target=\"_blank\">https://wandb.ai/hms-hslu-aicomp-hs25/hms-aicomp-gru-conv/runs/6qgvfyti</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- Epoch 1/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e4321cbe6646d18cbb0b712be6c11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3e9eede67b4737b7910041f43c15b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train Loss = 1.0686, Valid Loss = 1.0593, LR = 0.000100\n",
      "   New best model saved with validation loss: 1.0593\n",
      "   --- Epoch 2/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43f6a409543479d8d1dff38512f06be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5be01f952a47b9a2b0752191b5a498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train Loss = 0.8366, Valid Loss = 0.8711, LR = 0.000100\n",
      "   New best model saved with validation loss: 0.8711\n",
      "   --- Epoch 3/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8ff8c02a0148668d33a3499de73223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e34a23a46848809b2ccb3c57b7d3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train Loss = 0.7226, Valid Loss = 0.7882, LR = 0.000100\n",
      "   New best model saved with validation loss: 0.7882\n",
      "   --- Epoch 4/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d669160ccf4b45f8a435a3855712236e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7749f7bbc5824fc6919f63faca5f2744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train Loss = 0.6620, Valid Loss = 0.7714, LR = 0.000099\n",
      "   New best model saved with validation loss: 0.7714\n",
      "   --- Epoch 5/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd416554368844c69aedb0e86dedf3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8fb812104942399625a9c24e50107e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train Loss = 0.6344, Valid Loss = 0.7280, LR = 0.000098\n",
      "   New best model saved with validation loss: 0.7280\n",
      "   --- Epoch 6/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8baf341f7b474413b3b634a7bed9e536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a5270b044d423998399ad29b6eefd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 6: Train Loss = 0.6172, Valid Loss = 0.8886, LR = 0.000098\n",
      "   No improvement. Patience: 1/10\n",
      "   --- Epoch 7/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75267645590147cf812eb4b5543f4602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff1cc503322483fafaf5fe3ff8afefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 7: Train Loss = 0.5938, Valid Loss = 0.7421, LR = 0.000096\n",
      "   No improvement. Patience: 2/10\n",
      "   --- Epoch 8/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ec19d81cdb4086a60d8e056c83d66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c10e6bf535406086a1459e439a9c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 8: Train Loss = 0.5824, Valid Loss = 0.7430, LR = 0.000095\n",
      "   No improvement. Patience: 3/10\n",
      "   --- Epoch 9/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7234f8cff134443dbb299a0955e157e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c775e1f85007420585cd7f2085e84808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 9: Train Loss = 0.5585, Valid Loss = 0.7181, LR = 0.000094\n",
      "   New best model saved with validation loss: 0.7181\n",
      "   --- Epoch 10/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7450cd1e8a49f7a9bcd67a35f07ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e6ab34356344afa25bbd5873cc8689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 10: Train Loss = 0.5525, Valid Loss = 0.6874, LR = 0.000092\n",
      "   New best model saved with validation loss: 0.6874\n",
      "   --- Epoch 11/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48472d7ec2d1484c8565e004feca6008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619a2fab77524a34806178135545bfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 11: Train Loss = 0.5369, Valid Loss = 0.7309, LR = 0.000090\n",
      "   No improvement. Patience: 1/10\n",
      "   --- Epoch 12/50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d8126ecfde4dbe931230e47fc614c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    experiments_to_run = [3]  \n",
    "    \n",
    "    experiment_results = {}\n",
    "\n",
    "    print(f\"Starting systematic evaluation for num_cnn_blocks: {experiments_to_run}...\")\n",
    "\n",
    "    for num_blocks in experiments_to_run:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"      STARTING EXPERIMENT: {num_blocks} CNN BLOCKS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        CFG.num_cnn_blocks = num_blocks\n",
    "        \n",
    "\n",
    "        overall_cv_score, all_fold_scores = run_training(df, DATA_PREPARATION_VOTE_METHOD)\n",
    "        \n",
    "        mean_fold_score = np.mean(all_fold_scores)\n",
    "        \n",
    "        experiment_results[num_blocks] = {\n",
    "            'overall_cv_score': overall_cv_score,\n",
    "            'mean_fold_score': mean_fold_score\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"      FINISHED EXPERIMENT: {num_blocks} CNN BLOCKS\")\n",
    "        print(f\"      OOF KL Score (all folds): {overall_cv_score:.4f}\")\n",
    "        print(f\"      Mean of fold scores: {mean_fold_score:.4f}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "\n",
    "    for num_blocks, results in experiment_results.items():\n",
    "        print(f\"{num_blocks:<12} | {results['overall_cv_score']:<18.4f} | {results['mean_fold_score']:<18.4f}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"All experiments complete. Check wandb for detailed charts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef0533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF KL Score (calculated across all folds): 0.5988\n",
      "Mean of individual fold scores: 0.5970\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF KL Score (calculated across all folds): {overall_cv_score:.4f}\")\n",
    "print(f\"Mean of individual fold scores: {np.mean(all_fold_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d04c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
