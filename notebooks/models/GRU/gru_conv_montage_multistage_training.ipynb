{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f5188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maiko/miniconda3/envs/aicomp_312/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/maiko/miniconda3/envs/aicomp_312/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-12-02 21:24:31,587 :: root :: INFO :: Initialising Utils\n",
      "2025-12-02 21:24:32,167 :: root :: INFO :: Initialising Datasets\n",
      "2025-12-02 21:24:32,191 :: root :: INFO :: Initialising Models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping module cbramod_dataset due to missing dependency: No module named 'mne'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaikotrede\u001b[0m (\u001b[33mhms-hslu-aicomp-hs25\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm \n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "from src.utils.k_folds_creator import KFoldCreator\n",
    "from src.utils.utils import get_models_save_path\n",
    "from src.utils.constants import Constants \n",
    "from src.datasets.eeg_dataset_montage import EEGDatasetMontage\n",
    "from src.models.gru_convolution_attention import NodeAttentionModel\n",
    "\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    data_path = '../../../data/'\n",
    "\n",
    "    model_name = 'GRUConvNodeAttentionModel'\n",
    "    hidden_units = 256\n",
    "    num_layers = 1\n",
    "    target_size = 6 \n",
    "    \n",
    "    num_cnn_blocks = 3 \n",
    "    \n",
    "    sampling_rate = 200 # Hz\n",
    "    sequence_duration = 50 \n",
    "    downsample_factor = 1\n",
    "    \n",
    "    num_channels = 19\n",
    "    \n",
    "    dropout = 0.4\n",
    "    batch_size = 32\n",
    "    num_workers = 8\n",
    "    \n",
    "    folds_to_train = [0,1,2,3,4] #speficy list of folds to train [0,1,2,3,4]\n",
    "    \n",
    "\n",
    "    stage1_epochs = 50\n",
    "    stage1_lr = 10**-4\n",
    "    \n",
    "    stage2_epochs = 15\n",
    "    stage2_lr = 10**-4.5\n",
    "    \n",
    "    patience = 10  \n",
    "    min_delta = 0.001\n",
    "    \n",
    "    use_attention = True\n",
    "\n",
    "    use_mixup = True      \n",
    "    mixup_alpha = 0.5      \n",
    "    \n",
    "\n",
    "CFG.sequence_length = CFG.sequence_duration * CFG.sampling_rate \n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "TARGETS = Constants.TARGETS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deca2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, device='cuda'):\n",
    "    \"\"\"\n",
    "    Applies MixUp to inputs and targets.\n",
    "    Returns mixed inputs and mixed targets.\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    mixed_y = lam * y + (1 - lam) * y[index, :]\n",
    "    \n",
    "    return mixed_x, mixed_y\n",
    "\n",
    "\n",
    "def get_dataloaders(train_df, valid_df):\n",
    "    \"\"\"\n",
    "    Updated to handle empty DataFrames safely.\n",
    "    Returns None for the loader if the input DataFrame is empty.\n",
    "    \"\"\"\n",
    "    train_loader = None\n",
    "    valid_loader = None\n",
    "\n",
    "\n",
    "    if train_df is not None and not train_df.empty:\n",
    "        train_dataset = EEGDatasetMontage(\n",
    "            df=train_df, \n",
    "            data_path=CFG.data_path, \n",
    "            mode='train', \n",
    "            downsample_factor=CFG.downsample_factor, \n",
    "            augmentations=[\"channel_mask\", \"time_shift\"] \n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=CFG.batch_size, shuffle=True,\n",
    "            num_workers=CFG.num_workers, pin_memory=True, drop_last=True, \n",
    "            persistent_workers=True if CFG.num_workers > 0 else False\n",
    "        )\n",
    "    \n",
    "    if valid_df is not None and not valid_df.empty:\n",
    "        valid_dataset = EEGDatasetMontage(\n",
    "            df=valid_df, \n",
    "            data_path=CFG.data_path, \n",
    "            mode='valid', \n",
    "            downsample_factor=CFG.downsample_factor\n",
    "        )\n",
    "        \n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=CFG.batch_size, shuffle=False,\n",
    "            num_workers=CFG.num_workers, pin_memory=True, drop_last=False, \n",
    "            persistent_workers=True if CFG.num_workers > 0 else False\n",
    "        )\n",
    "    \n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d909a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_stage(fold, stage_name, train_df, valid_df, group_name, starting_checkpoint=None):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    autocast_enabled = (device.type == 'cuda')\n",
    "    \n",
    "    if stage_name == \"Stage1\":\n",
    "        lr = CFG.stage1_lr\n",
    "        epochs = CFG.stage1_epochs\n",
    "    else:\n",
    "        lr = CFG.stage2_lr\n",
    "        epochs = CFG.stage2_epochs\n",
    "        \n",
    "    print(f\"\\n--- Starting {stage_name} | Fold {fold} ---\")\n",
    "    \n",
    "\n",
    "    experiment_name = f\"{group_name}_{stage_name}_fold{fold}\"\n",
    "    \n",
    "    config = {\n",
    "        \"architecture\": CFG.model_name,\n",
    "        \"fold\": fold, \n",
    "        \"stage\": stage_name,\n",
    "        \"optimizer\": \"AdamW\", \n",
    "        \"learning_rate\": lr, \n",
    "        \"batch_size\": CFG.batch_size,\n",
    "        \"epochs\": epochs, \n",
    "        \"num_cnn_blocks\": CFG.num_cnn_blocks,\n",
    "        \"use_attention\": CFG.use_attention,\n",
    "        \"seed\": CFG.seed\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"hms-aicomp-gru-conv\",\n",
    "        name=experiment_name,\n",
    "        group=group_name, \n",
    "        job_type=stage_name,\n",
    "        tags=['two-stage', stage_name, f'fold{fold}', f'blocks_{CFG.num_cnn_blocks}'],\n",
    "        config=config,\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "\n",
    "    model = NodeAttentionModel(\n",
    "        num_nodes=CFG.num_channels,       \n",
    "        node_embed_size=256,              \n",
    "        hidden_size=CFG.hidden_units,    \n",
    "        num_layers=CFG.num_layers,       \n",
    "        num_classes=CFG.target_size,     \n",
    "        num_cnn_blocks=CFG.num_cnn_blocks,\n",
    "        dropout=CFG.dropout,\n",
    "        use_inception=True              \n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    if starting_checkpoint:\n",
    "        print(f\"Loading weights from {starting_checkpoint}...\")\n",
    "        model.load_state_dict(torch.load(starting_checkpoint))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
    "    scaler = GradScaler(enabled=autocast_enabled)\n",
    "\n",
    "    train_loader, valid_loader = get_dataloaders(train_df, valid_df)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = get_models_save_path() / \"TwoStage\" / f\"{stage_name}_fold{fold}.pth\"\n",
    "    best_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for signals, labels in tqdm(train_loader, desc=f\"{stage_name} E{epoch+1}\", leave=False):\n",
    "            signals, labels = signals.to(device), labels.to(device)\n",
    "            \n",
    "            if CFG.use_mixup:\n",
    "                signals, labels = mixup_data(signals, labels, alpha=CFG.mixup_alpha, device=device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with autocast(enabled=autocast_enabled, device_type=device.type):\n",
    "                outputs = model(signals)\n",
    "                log_probs = F.log_softmax(outputs, dim=1)\n",
    "                loss = loss_fn(log_probs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item() * signals.size(0)\n",
    "            \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for signals, labels in valid_loader:\n",
    "                signals, labels = signals.to(device), labels.to(device)\n",
    "                with autocast(enabled=autocast_enabled, device_type=device.type):\n",
    "                    outputs = model(signals)\n",
    "                    log_probs = F.log_softmax(outputs, dim=1)\n",
    "                    loss = loss_fn(log_probs, labels)\n",
    "                valid_loss += loss.item() * signals.size(0)\n",
    "\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "\n",
    "        epoch_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"  Ep {epoch+1}: Train={train_loss:.4f} | Val={valid_loss:.4f} | LR={epoch_lr:.6f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1, \n",
    "            \"train/epoch_loss\": train_loss,   \n",
    "            \"val/loss\": valid_loss,          \n",
    "            \"val/kl_div\": valid_loss,        \n",
    "            \"train/epoch_lr\": epoch_lr       \n",
    "        })\n",
    "\n",
    "        if valid_loss < best_val_loss - CFG.min_delta:\n",
    "            best_val_loss = valid_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= CFG.patience:\n",
    "            print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "            break \n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "    wandb.finish()\n",
    "    return best_model_path, best_val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc4fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_two_stage_pipeline(df):\n",
    "\n",
    "    print(\"Total Votes Distribution (Head):\")\n",
    "    print(df['total_votes'].head())\n",
    "    \n",
    "    mask_low_votes = df['total_votes'] < 10\n",
    "    mask_high_votes = df['total_votes'] >= 10\n",
    "    \n",
    "    print(f\"Stage 1 Data (Low Votes < 10): {mask_low_votes.sum()} samples\")\n",
    "    print(f\"Stage 2 Data (High Votes >= 10): {mask_high_votes.sum()} samples\")\n",
    "    group_name = f\"TwoStage_montages_block_{CFG.num_cnn_blocks}_attention_{CFG.use_attention}_AUG(MU+CU)\"\n",
    "    \n",
    "    all_oof_preds = []\n",
    "    all_oof_labels = []\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"\\n{'='*20} Processing FOLD {fold} {'='*20}\")\n",
    "        if fold not in CFG.folds_to_train:\n",
    "            print(f\"Skipping Fold {fold}...\")\n",
    "            continue\n",
    "        valid_idx = df['fold'] == fold\n",
    "        valid_df = df[valid_idx].reset_index(drop=True)\n",
    "        \n",
    "        valid_stage2_df = valid_df[valid_df['total_votes'] >= 10].reset_index(drop=True)\n",
    "\n",
    "        train_stage1 = df[(df['fold'] != fold) & mask_low_votes].reset_index(drop=True)\n",
    "        \n",
    "        stage1_path, _ = train_one_stage(\n",
    "            fold=fold,\n",
    "            stage_name=\"Stage1\",\n",
    "            train_df=train_stage1,\n",
    "            valid_df=valid_df,\n",
    "            group_name=group_name \n",
    "        )\n",
    "\n",
    "        train_stage2 = df[(df['fold'] != fold) & mask_high_votes].reset_index(drop=True)\n",
    "        \n",
    "        stage2_path, best_val_loss = train_one_stage(\n",
    "            fold=fold,\n",
    "            stage_name=\"Stage2\",\n",
    "            train_df=train_stage2,\n",
    "            valid_df=valid_stage2_df, \n",
    "            starting_checkpoint=stage1_path,\n",
    "            group_name=group_name\n",
    "        )\n",
    "        \n",
    "        fold_scores.append(best_val_loss)\n",
    "\n",
    "        print(f\"Generating OOF predictions for Fold {fold}...\")\n",
    "        device = torch.device('cuda')\n",
    "        model = NodeAttentionModel(\n",
    "            num_nodes=CFG.num_channels, node_embed_size=256, hidden_size=CFG.hidden_units,    \n",
    "            num_layers=CFG.num_layers, num_classes=CFG.target_size, num_cnn_blocks=CFG.num_cnn_blocks,\n",
    "            dropout=CFG.dropout, use_inception=True              \n",
    "        )\n",
    "        model.load_state_dict(torch.load(stage2_path))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        _, valid_loader = get_dataloaders(pd.DataFrame(), valid_df)\n",
    "        \n",
    "        probs_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for signals, labels in valid_loader:\n",
    "                signals = signals.to(device)\n",
    "                outputs = model(signals)\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                probs_list.append(probs)\n",
    "                labels_list.append(labels.numpy())\n",
    "        \n",
    "        all_oof_preds.append(np.concatenate(probs_list))\n",
    "        all_oof_labels.append(np.concatenate(labels_list))\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        run = wandb.init(project=\"hms-aicomp-gru-conv\", job_type=\"artifact_upload\", name=f\"artifact_fold{fold}\")\n",
    "        artifact = wandb.Artifact(f'model-fold{fold}-stage2', type='model')\n",
    "        artifact.add_file(stage2_path)\n",
    "        wandb.log_artifact(artifact)\n",
    "        wandb.finish()\n",
    "\n",
    "    all_oof_preds = np.concatenate(all_oof_preds)\n",
    "    all_oof_labels = np.concatenate(all_oof_labels)\n",
    "    \n",
    "    oof_tensor = torch.tensor(all_oof_preds, dtype=torch.float32)\n",
    "    true_tensor = torch.tensor(all_oof_labels, dtype=torch.float32)\n",
    "    \n",
    "    oof_tensor = torch.clamp(oof_tensor, 1e-6, 1.0)\n",
    "    \n",
    "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    overall_score = kl_loss(torch.log(oof_tensor), true_tensor).item()\n",
    "    \n",
    "    return overall_score, fold_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb66574",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Loading Data...\")\n",
    "    df = pd.read_csv(CFG.data_path + 'processed_data_max_vote_window.csv') \n",
    "\n",
    "    if 'expert_consensus' not in df.columns:\n",
    "        df['expert_consensus'] = df[TARGETS].idxmax(axis=1)\n",
    "\n",
    "    print('Train shape:', df.shape)\n",
    "\n",
    "    fold_creator = KFoldCreator(n_splits=CFG.n_splits, seed=CFG.seed)\n",
    "    df = fold_creator.create_folds(df, stratify_col='expert_consensus', group_col='patient_id')\n",
    "\n",
    "    overall_cv, fold_scores = run_two_stage_pipeline(df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Overall OOF KL-Divergence: {overall_cv:.4f}\")\n",
    "    print(f\"Average Fold Score: {np.mean(fold_scores):.4f}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ded4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
