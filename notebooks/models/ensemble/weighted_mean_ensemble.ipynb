{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTING_CHECKPOINT_KAGGLE_DATASET_ID = \"hsm-models\"\n",
    "DATA_PREPARATION_VOTE_METHOD = \"max_vote_window\" # \"max_vote_window\" or \"sum_and_normalize\". Decides how to aggregate the predictions of the overlapping windows\n",
    "DATA_SOURCE = \"cv\" # \"cv\" or \"test\". On Kaggle, this is autpmatically set to \"test\"\n",
    "WEIGHTS = [] # Pre-computed weights for each model in the ensemble. For CV, the weights are always computed automatically. For test, they must be provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 15:03:38,180 :: root :: INFO :: Initialising Utils\n",
      "2025-11-29 15:03:38,951 :: root :: INFO :: Initialising Datasets\n",
      "2025-11-29 15:03:39,992 :: root :: INFO :: Initialising Models\n",
      "/home/david/miniconda3/envs/aicomp/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/david/miniconda3/envs/aicomp/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "if bool(os.environ.get(\"KAGGLE_URL_BASE\", \"\")):\n",
    "  import sys\n",
    "  # running on kaggle\n",
    "  sys.path.insert(0, \"/kaggle/input/hsm-source-files\")\n",
    "else:\n",
    "  # running locally\n",
    "  sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "\n",
    "from src.utils.utils import get_raw_data_dir, get_processed_data_dir, get_submission_csv_path, get_models_save_path, set_seeds, running_in_kaggle\n",
    "from src.utils.constants import Constants\n",
    "\n",
    "from src.utils.k_folds_creator import KFoldCreator\n",
    "from src.datasets.multi_spectrogram import MultiSpectrogramDataset\n",
    "from src.datasets.eeg_dataset_montage import EEGDatasetMontage\n",
    "from src.models.base_cnn import BaseCNN\n",
    "from src.models.gru_convolution_attention import NodeAttentionModel\n",
    "from src.utils.eeg_spectrogram_creator import EEGSpectrogramGenerator\n",
    "from src.datasets.eeg_processor import EEGDataProcessor\n",
    "\n",
    "set_seeds(Constants.SEED)\n",
    "\n",
    "if running_in_kaggle():\n",
    "  DATA_SOURCE = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eeg_spectrograms(eeg_spectrograms_path, raw_eegs_path, data_df):\n",
    "  os.makedirs(eeg_spectrograms_path, exist_ok=True)\n",
    "  existing_specs = len(list(eeg_spectrograms_path.glob(\"*.npy\")))\n",
    "\n",
    "  eeg_ids = data_df[\"eeg_id\"].unique()\n",
    "  if existing_specs >= len(eeg_ids):\n",
    "    print(\"EEG Spectrograms already created.\")\n",
    "    return\n",
    "  else:\n",
    "    spectrogram_creator = EEGSpectrogramGenerator([\"cwt\"])\n",
    "    for eeg_id in tqdm(eeg_ids, desc=\"Generating EEG Spectrograms\"):\n",
    "        eeg_path = os.path.join(raw_eegs_path, f\"{eeg_id}.parquet\")\n",
    "        eeg = pd.read_parquet(eeg_path)\n",
    "        spectrograms = spectrogram_creator.generate(eeg)\n",
    "        np.save(eeg_spectrograms_path / f\"{eeg_id}.npy\", spectrograms['cwt']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor initialized.\n",
      "Raw data path: '/home/david/git/aicomp/data'\n",
      "Processed data path: '/home/david/git/aicomp/data/processed'\n",
      "==================================================\n",
      "Starting EEG Data Processing Pipeline\n",
      "==================================================\n",
      "Skipping Parquet file creation as requested.\n",
      "Using 'max_vote_window' vote aggregation strategy.\n",
      "\n",
      "Processed train data saved to '/home/david/git/aicomp/data/processed/train_processed.csv'.\n",
      "Shape of the final dataframe: (17089, 12)\n",
      "\n",
      "Pipeline finished successfully!\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>min_offset</th>\n",
       "      <th>max_offset</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568657</td>\n",
       "      <td>789577333</td>\n",
       "      <td>20654</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582999</td>\n",
       "      <td>1552638400</td>\n",
       "      <td>20230</td>\n",
       "      <td>LPD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>642382</td>\n",
       "      <td>14960202</td>\n",
       "      <td>5955</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751790</td>\n",
       "      <td>618728447</td>\n",
       "      <td>38549</td>\n",
       "      <td>GPD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>908.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778705</td>\n",
       "      <td>52296320</td>\n",
       "      <td>40955</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eeg_id  spectrogram_id  patient_id expert_consensus  seizure_vote  \\\n",
       "0  568657       789577333       20654            Other           0.0   \n",
       "1  582999      1552638400       20230              LPD           0.0   \n",
       "2  642382        14960202        5955            Other           0.0   \n",
       "3  751790       618728447       38549              GPD           0.0   \n",
       "4  778705        52296320       40955            Other           0.0   \n",
       "\n",
       "   lpd_vote  gpd_vote  lrda_vote  grda_vote  other_vote  min_offset  \\\n",
       "0  0.000000      0.25   0.000000   0.166667    0.583333         0.0   \n",
       "1  0.857143      0.00   0.071429   0.000000    0.071429         0.0   \n",
       "2  0.000000      0.00   0.000000   0.000000    1.000000      1008.0   \n",
       "3  0.000000      1.00   0.000000   0.000000    0.000000       908.0   \n",
       "4  0.000000      0.00   0.000000   0.000000    1.000000         0.0   \n",
       "\n",
       "   max_offset  fold  \n",
       "0        16.0     1  \n",
       "1        38.0     1  \n",
       "2      1032.0     1  \n",
       "3       908.0     1  \n",
       "4         0.0     4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = get_raw_data_dir()\n",
    "\n",
    "if DATA_SOURCE == \"cv\":\n",
    "  EEG_SPECT_PATH = get_processed_data_dir() / \"eeg_spectrograms\" / \"train\" / \"cwt\"\n",
    "  processor = EEGDataProcessor(raw_data_path=DATA_PATH, processed_data_path=get_processed_data_dir())\n",
    "  data_df = processor.process_data(vote_method=DATA_PREPARATION_VOTE_METHOD, skip_parquet=True)\n",
    "\n",
    "  fold_creator = KFoldCreator(n_splits=5, seed=Constants.SEED)\n",
    "  data_df = fold_creator.create_folds(data_df, stratify_col='expert_consensus', group_col='patient_id')\n",
    "else:\n",
    "  EEG_SPECT_PATH = get_processed_data_dir() / \"eeg_spectrograms\" / \"test\" / \"cwt\"\n",
    "  data_df = pd.read_csv(DATA_PATH / \"test.csv\")\n",
    "  create_eeg_spectrograms(EEG_SPECT_PATH, DATA_PATH / \"test_eegs\", data_df)\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_spect_config = {\n",
    "  \"batch_size\": 64,\n",
    "  \"num_workers\": 8,\n",
    "  \"pretrained_model_name\": \"inception_v3\",\n",
    "  \"target_size\": 6,\n",
    "  \"img_size\": (128, 256), \n",
    "  \"dropout_p\": 0.1,\n",
    "  \"image_alignment\": \"stacked\",\n",
    "  \"augmentations\": []\n",
    "}\n",
    "\n",
    "multi_spect_model = BaseCNN(\n",
    "  multi_spect_config[\"pretrained_model_name\"],\n",
    "  pretrained=False,\n",
    "  num_classes=multi_spect_config[\"target_size\"],\n",
    "  dropout_p=multi_spect_config[\"dropout_p\"],\n",
    "  image_alignment=multi_spect_config[\"image_alignment\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_conv_montage_config = {\n",
    "  \"batch_size\": 32,\n",
    "  \"num_workers\": 8,\n",
    "  \"num_nodes\": 19,\n",
    "  \"node_embed_size\": 256,\n",
    "  \"hidden_size\": 256,\n",
    "  \"num_layers\": 1,\n",
    "  \"target_size\": 6,\n",
    "  \"num_cnn_blocks\": 3,\n",
    "  \"dropout\": 0.4,\n",
    "  \"downsample_factor\": 1,\n",
    "  \"use_inception\": True,\n",
    "  \"augmentations\": []\n",
    "}\n",
    "\n",
    "gru_conv_montage_model = NodeAttentionModel(\n",
    "  num_nodes=gru_conv_montage_config[\"num_nodes\"],\n",
    "  node_embed_size=gru_conv_montage_config[\"node_embed_size\"],\n",
    "  hidden_size=gru_conv_montage_config[\"hidden_size\"],\n",
    "  num_layers=gru_conv_montage_config[\"num_layers\"],\n",
    "  num_classes=gru_conv_montage_config[\"target_size\"],\n",
    "  num_cnn_blocks=gru_conv_montage_config[\"num_cnn_blocks\"],\n",
    "  dropout=gru_conv_montage_config[\"dropout\"],\n",
    "  use_inception=gru_conv_montage_config[\"use_inception\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "  {\n",
    "    \"identifier\": \"multi-spect-cnn\",\n",
    "    \"config\": multi_spect_config,\n",
    "    \"model\": multi_spect_model,\n",
    "    \"model_checkpoints_dir\": get_models_save_path(EXISTING_CHECKPOINT_KAGGLE_DATASET_ID) / \"multi_spec_cnn\" / \"inception_v3\" / DATA_PREPARATION_VOTE_METHOD,\n",
    "    \"autocast_enabled\": False,\n",
    "    \"dataset_creator\": lambda df, mode, augmentations: MultiSpectrogramDataset(\n",
    "      df=df,\n",
    "      targets=Constants.TARGETS,\n",
    "      data_path=DATA_PATH,\n",
    "      img_size=multi_spect_config[\"img_size\"],\n",
    "      eeg_spec_path=EEG_SPECT_PATH,\n",
    "      mode=mode,\n",
    "      apply_augmentations=augmentations\n",
    "    )\n",
    "  },\n",
    "  {\n",
    "    \"identifier\": \"gru_conv_montage\",\n",
    "    \"config\": gru_conv_montage_config,\n",
    "    \"model\": gru_conv_montage_model,\n",
    "    \"model_checkpoints_dir\": get_models_save_path(EXISTING_CHECKPOINT_KAGGLE_DATASET_ID) / \"GRUConvModel\" / DATA_PREPARATION_VOTE_METHOD,\n",
    "    \"autocast_enabled\": device.type == 'cuda',\n",
    "    \"dataset_creator\": lambda df, mode, augmentations: EEGDatasetMontage(\n",
    "      df=df,\n",
    "      data_path=DATA_PATH,\n",
    "      mode=mode,\n",
    "      downsample_factor=gru_conv_montage_config[\"downsample_factor\"],\n",
    "      augmentations=augmentations\n",
    "    )\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_predictions(all_model_predictions, weights):\n",
    "    # Returns weighted average of predictions\n",
    "    weights = np.array(weights) / np.sum(weights) # normalize weights to sum to 1\n",
    "    ensemble_pred = np.zeros_like(all_model_predictions[0])\n",
    "    for i, preds in enumerate(all_model_predictions):\n",
    "        ensemble_pred += weights[i] * preds\n",
    "    return ensemble_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kl_score(true_labels, pred_labels):\n",
    "  kl_loss_fn = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "  return kl_loss_fn(torch.log(pred_labels + 1e-8), true_labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ensemble_weights(all_model_predictions, true_labels, method, max_iterations):\n",
    "    n_models = len(all_model_predictions)\n",
    "    \n",
    "    def objective(weights):\n",
    "        # Weighted average of predictions\n",
    "        ensemble_pred = get_ensemble_predictions(all_model_predictions, weights)\n",
    "        \n",
    "        # Calculate KL divergence\n",
    "        pred_tensor = torch.tensor(ensemble_pred, dtype=torch.float32)\n",
    "        true_tensor = torch.tensor(true_labels, dtype=torch.float32)\n",
    "        return calculate_kl_score(true_tensor, pred_tensor)\n",
    "\n",
    "    # Initial weights (equal for all models)\n",
    "    initial_weights = np.ones(n_models) / n_models\n",
    "    \n",
    "    # Bounds: each weight between 0 and 1\n",
    "    bounds = [(0, 1) for _ in range(n_models)]\n",
    "    \n",
    "    # Optimize\n",
    "    print(f\"Optimizing weights for {n_models} models...\")\n",
    "    print(f\"Initial equal weights: {initial_weights}\")\n",
    "    print(f\"Initial KL score: {objective(initial_weights):.6f}\")\n",
    "    \n",
    "    result = minimize(\n",
    "        objective,\n",
    "        initial_weights,\n",
    "        method=method,\n",
    "        bounds=bounds,\n",
    "        options={\"maxiter\": max_iterations}\n",
    "    )\n",
    "    \n",
    "    print(result)\n",
    "\n",
    "    optimal_weights = result.x / result.x.sum()  # Normalize weights to sum to 1\n",
    "    best_score = result.fun\n",
    "    \n",
    "    print(f\"Optimized weights: {optimal_weights}\")\n",
    "    print(f\"Optimized KL score: {best_score:.6f}\")\n",
    "    print(f\"Improvement: {objective(initial_weights) - best_score:.6f}\")\n",
    "    \n",
    "    return optimal_weights, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_weights(all_model_predictions, true_labels, n_steps=500):\n",
    "    \"\"\"Brute force grid search for 2 models\"\"\"\n",
    "    n_models = len(all_model_predictions)\n",
    "    \n",
    "    if n_models != 2:\n",
    "        raise ValueError(\"Grid search only implemented for 2 models\")\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_weights = None\n",
    "    \n",
    "    print(f\"Grid searching with {n_steps} steps...\")\n",
    "    \n",
    "    for i in range(n_steps + 1):\n",
    "        w1 = i / n_steps\n",
    "        w2 = 1 - w1\n",
    "        weights = np.array([w1, w2])\n",
    "        \n",
    "        ensemble_pred = get_ensemble_predictions(all_model_predictions, weights)\n",
    "        pred_tensor = torch.tensor(ensemble_pred, dtype=torch.float32)\n",
    "        true_tensor = torch.tensor(true_labels, dtype=torch.float32)\n",
    "        score = calculate_kl_score(true_tensor, pred_tensor)\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_weights = weights\n",
    "            print(f\"New best - Weights: [{w1:.2f}, {w2:.2f}], Score: {score:.6f}\")\n",
    "    \n",
    "    print(f\"\\nBest weights: {best_weights}\")\n",
    "    print(f\"Best score: {best_score:.6f}\")\n",
    "    \n",
    "    return best_weights, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble_inference_cv():\n",
    "  \"\"\"Run OOF inference for CV data. Each fold predicts only its validation set\"\"\"\n",
    "  all_model_predictions = []\n",
    "\n",
    "  for config in model_configs:\n",
    "    print(f\"\\n========== Loading {config['identifier']} ==========\")\n",
    "\n",
    "    model_checkpoints_dir = config[\"model_checkpoints_dir\"]\n",
    "    model_paths = [os.path.join(model_checkpoints_dir, f'best_model_fold{i}.pth') for i in range(5)]\n",
    "\n",
    "    # Initialize array to store OOF predictions for this model\n",
    "    model_oof_preds = np.zeros((len(data_df), len(Constants.TARGETS)))\n",
    "    \n",
    "    for fold_idx in range(5):\n",
    "      print(f\"\\n========== Inferencing with Fold {fold_idx} Model ==========\")\n",
    "      model_path = model_paths[fold_idx]\n",
    "      \n",
    "      if not os.path.exists(model_path):\n",
    "          print(f\"Model file not found: {model_path}. Skipping this fold.\")\n",
    "          continue\n",
    "      \n",
    "      # Get validation indices and data for this fold\n",
    "      valid_df = data_df[data_df['fold'] == fold_idx].reset_index(drop=True)\n",
    "      valid_indices = data_df[data_df['fold'] == fold_idx].index.tolist()\n",
    "      \n",
    "      # Create dataset using lambda function\n",
    "      fold_dataset = config['dataset_creator'](valid_df, mode='train', augmentations=config['config']['augmentations'])\n",
    "      \n",
    "      fold_loader = DataLoader(\n",
    "          fold_dataset,\n",
    "          batch_size=config['config']['batch_size'],\n",
    "          shuffle=False,\n",
    "          num_workers=config['config']['num_workers']\n",
    "      )\n",
    "      \n",
    "      # Load model\n",
    "      model = config['model']\n",
    "      model.load_state_dict(torch.load(model_path))\n",
    "      model.to(device)\n",
    "      model.eval()\n",
    "\n",
    "      fold_preds = []\n",
    "      with torch.no_grad():\n",
    "          for x in tqdm(fold_loader, desc=f\"{config['identifier']} Fold {fold_idx}\"):\n",
    "              if isinstance(x, (list, tuple)):\n",
    "                  x = x[0]\n",
    "\n",
    "              x = x.to(device)\n",
    "\n",
    "              with torch.autocast(enabled=config[\"autocast_enabled\"], device_type=device.type):\n",
    "                outputs = model(x)\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                fold_preds.append(probs)\n",
    "      \n",
    "      # Store predictions at the correct indices\n",
    "      fold_preds = np.concatenate(fold_preds)\n",
    "      model_oof_preds[valid_indices] = fold_preds\n",
    "      \n",
    "      print(f\"Completed fold {fold_idx} for {config['identifier']}: {fold_preds.shape}\")\n",
    "    \n",
    "    all_model_predictions.append(model_oof_preds)\n",
    "    print(f\"Completed {config['identifier']}: {model_oof_preds.shape}\")\n",
    "\n",
    "  true_labels = data_df[Constants.TARGETS].values\n",
    "  optimal_weights, _ = grid_search_weights(\n",
    "      all_model_predictions, \n",
    "      true_labels,\n",
    "      n_steps=100\n",
    "  )\n",
    "\n",
    "  # Simple average across all models\n",
    "  print(\"\\n========== Combining Model Predictions ==========\")\n",
    "  final_predictions = get_ensemble_predictions(all_model_predictions, weights=optimal_weights)\n",
    "\n",
    "  submission = pd.DataFrame({\"eeg_id\": data_df[\"eeg_id\"]})\n",
    "  submission[Constants.TARGETS] = final_predictions\n",
    "\n",
    "  return submission, all_model_predictions\n",
    "\n",
    "\n",
    "def run_ensemble_inference_test(weights):\n",
    "  \"\"\"Run inference on test data with optional pre-computed weights\"\"\"\n",
    "  assert weights is not None or len(weights) == 0, \"Weights must be provided for test inference\"\n",
    "\n",
    "  all_model_predictions = []\n",
    "\n",
    "  for config in model_configs:\n",
    "    print(f\"\\n========== Loading {config['identifier']} ==========\")\n",
    "\n",
    "    model_checkpoints_dir = config[\"model_checkpoints_dir\"]\n",
    "    model_paths = [os.path.join(model_checkpoints_dir, f'best_model_fold{i}.pth') for i in range(5)]\n",
    "\n",
    "    dataset = config['dataset_creator'](data_df, mode='test', augmentations=config['config']['augmentations'])\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config['config']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['config']['num_workers']\n",
    "    )\n",
    "\n",
    "    # Get predictions from all folds for this model\n",
    "    fold_predictions = []\n",
    "    for i, path in enumerate(model_paths):\n",
    "      print(f\"\\n========== Inferencing with Fold {i} Model ==========\")\n",
    "      if not os.path.exists(path):\n",
    "          print(f\"Model file not found: {path}. Skipping this fold.\")\n",
    "          continue\n",
    "      \n",
    "      model = config['model']\n",
    "      model.load_state_dict(torch.load(path))\n",
    "      model.to(device)\n",
    "      model.eval()\n",
    "\n",
    "      current_fold_preds = []\n",
    "      with torch.no_grad():\n",
    "          for x in tqdm(data_loader, desc=f\"{config['identifier']} Fold {i}\"):\n",
    "              x = x.to(device)\n",
    "\n",
    "              with torch.autocast(enabled=config[\"autocast_enabled\"], device_type=device.type):\n",
    "                outputs = model(x)\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                current_fold_preds.append(probs)\n",
    "            \n",
    "      fold_predictions.append(np.concatenate(current_fold_preds))\n",
    "\n",
    "    # Average across folds for this model\n",
    "    model_avg = np.mean(fold_predictions, axis=0)\n",
    "    all_model_predictions.append(model_avg)\n",
    "    print(f\"Completed {config['identifier']}: {model_avg.shape}\")\n",
    "\n",
    "  # Calculate weighted mean across all models\n",
    "  print(\"\\n========== Combining Model Predictions ==========\")\n",
    "  final_predictions = get_ensemble_predictions(all_model_predictions, weights)\n",
    "    \n",
    "  submission = pd.DataFrame({\"eeg_id\": data_df[\"eeg_id\"]})\n",
    "  submission[Constants.TARGETS] = final_predictions\n",
    "  submission.to_csv(get_submission_csv_path(), index=False)\n",
    "\n",
    "  return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Loading multi-spect-cnn ==========\n",
      "\n",
      "========== Inferencing with Fold 0 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-spect-cnn Fold 0: 100%|██████████| 64/64 [01:15<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fold 0 for multi-spect-cnn: (4067, 6)\n",
      "\n",
      "========== Inferencing with Fold 1 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-spect-cnn Fold 1: 100%|██████████| 58/58 [01:09<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fold 1 for multi-spect-cnn: (3658, 6)\n",
      "\n",
      "========== Inferencing with Fold 2 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-spect-cnn Fold 2: 100%|██████████| 53/53 [01:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fold 2 for multi-spect-cnn: (3381, 6)\n",
      "\n",
      "========== Inferencing with Fold 3 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-spect-cnn Fold 3: 100%|██████████| 42/42 [00:52<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fold 3 for multi-spect-cnn: (2625, 6)\n",
      "\n",
      "========== Inferencing with Fold 4 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi-spect-cnn Fold 4: 100%|██████████| 53/53 [01:08<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fold 4 for multi-spect-cnn: (3358, 6)\n",
      "Completed multi-spect-cnn: (17089, 6)\n",
      "\n",
      "========== Loading gru_conv_montage ==========\n",
      "\n",
      "========== Inferencing with Fold 0 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gru_conv_montage Fold 0: 100%|██████████| 128/128 [00:42<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fold 0 for gru_conv_montage: (4067, 6)\n",
      "\n",
      "========== Inferencing with Fold 1 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gru_conv_montage Fold 1: 100%|██████████| 115/115 [00:29<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fold 1 for gru_conv_montage: (3658, 6)\n",
      "\n",
      "========== Inferencing with Fold 2 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gru_conv_montage Fold 2: 100%|██████████| 106/106 [00:28<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fold 2 for gru_conv_montage: (3381, 6)\n",
      "\n",
      "========== Inferencing with Fold 3 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gru_conv_montage Fold 3: 100%|██████████| 83/83 [00:27<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fold 3 for gru_conv_montage: (2625, 6)\n",
      "\n",
      "========== Inferencing with Fold 4 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gru_conv_montage Fold 4: 100%|██████████| 105/105 [01:04<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed fold 4 for gru_conv_montage: (3358, 6)\n",
      "Completed gru_conv_montage: (17089, 6)\n",
      "Optimizing weights for 2 models...\n",
      "Initial equal weights: [0.5 0.5]\n",
      "Initial KL score: 0.551284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48776/3150423393.py:36: RuntimeWarning: Method L-BFGS-B cannot handle constraints.\n",
      "  result = minimize(\n",
      "/tmp/ipykernel_48776/3150423393.py:36: DeprecationWarning: scipy.optimize: The `disp` and `iprint` options of the L-BFGS-B solver are deprecated and will be removed in SciPy 1.18.0.\n",
      "  result = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 0.5512837767601013\n",
      "        x: [ 5.000e-01  5.000e-01]\n",
      "      nit: 0\n",
      "      jac: [ 0.000e+00  0.000e+00]\n",
      "     nfev: 3\n",
      "     njev: 1\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "Optimized weights: [0.5 0.5]\n",
      "Optimized KL score: 0.551284\n",
      "Improvement: 0.000000\n",
      "\n",
      "========== Combining Model Predictions ==========\n"
     ]
    }
   ],
   "source": [
    "if DATA_SOURCE == \"cv\":\n",
    "  submission, all_model_predictions = run_ensemble_inference_cv()\n",
    "else:\n",
    "  submission = run_ensemble_inference_test(WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid searching with 100 steps...\n",
      "New best - Weights: [0.00, 1.00], Score: 0.588282\n",
      "New best - Weights: [0.01, 0.99], Score: 0.583548\n",
      "New best - Weights: [0.02, 0.98], Score: 0.579754\n",
      "New best - Weights: [0.03, 0.97], Score: 0.576476\n",
      "New best - Weights: [0.04, 0.96], Score: 0.573566\n",
      "New best - Weights: [0.05, 0.95], Score: 0.570944\n",
      "New best - Weights: [0.06, 0.94], Score: 0.568559\n",
      "New best - Weights: [0.07, 0.93], Score: 0.566375\n",
      "New best - Weights: [0.08, 0.92], Score: 0.564368\n",
      "New best - Weights: [0.09, 0.91], Score: 0.562516\n",
      "New best - Weights: [0.10, 0.90], Score: 0.560805\n",
      "New best - Weights: [0.11, 0.89], Score: 0.559222\n",
      "New best - Weights: [0.12, 0.88], Score: 0.557756\n",
      "New best - Weights: [0.13, 0.87], Score: 0.556398\n",
      "New best - Weights: [0.14, 0.86], Score: 0.555141\n",
      "New best - Weights: [0.15, 0.85], Score: 0.553979\n",
      "New best - Weights: [0.16, 0.84], Score: 0.552905\n",
      "New best - Weights: [0.17, 0.83], Score: 0.551914\n",
      "New best - Weights: [0.18, 0.82], Score: 0.551004\n",
      "New best - Weights: [0.19, 0.81], Score: 0.550168\n",
      "New best - Weights: [0.20, 0.80], Score: 0.549405\n",
      "New best - Weights: [0.21, 0.79], Score: 0.548710\n",
      "New best - Weights: [0.22, 0.78], Score: 0.548082\n",
      "New best - Weights: [0.23, 0.77], Score: 0.547517\n",
      "New best - Weights: [0.24, 0.76], Score: 0.547014\n",
      "New best - Weights: [0.25, 0.75], Score: 0.546571\n",
      "New best - Weights: [0.26, 0.74], Score: 0.546184\n",
      "New best - Weights: [0.27, 0.73], Score: 0.545854\n",
      "New best - Weights: [0.28, 0.72], Score: 0.545578\n",
      "New best - Weights: [0.29, 0.71], Score: 0.545354\n",
      "New best - Weights: [0.30, 0.70], Score: 0.545182\n",
      "New best - Weights: [0.31, 0.69], Score: 0.545061\n",
      "New best - Weights: [0.32, 0.68], Score: 0.544988\n",
      "New best - Weights: [0.33, 0.67], Score: 0.544964\n",
      "\n",
      "Best weights: [0.33 0.67]\n",
      "Best score: 0.544964\n"
     ]
    }
   ],
   "source": [
    "# true_labels = data_df[Constants.TARGETS].values\n",
    "\n",
    "# # optimal_weights, _ = optimize_ensemble_weights(\n",
    "# #     all_model_predictions, \n",
    "# #     true_labels,\n",
    "# #     method=\"SLSQP\",\n",
    "# #     max_iterations=100\n",
    "# # )\n",
    "\n",
    "# optimal_weights, _ = grid_search_weights(\n",
    "#     all_model_predictions, \n",
    "#     true_labels,\n",
    "#     n_steps=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence Score on CV Data: 0.5512837767601013\n"
     ]
    }
   ],
   "source": [
    "# calculate KL divergence score if using cv data\n",
    "if DATA_SOURCE == \"cv\":\n",
    "  true_labels = torch.tensor(data_df[Constants.TARGETS].values, dtype=torch.float32)\n",
    "  pred_labels = torch.tensor(submission[Constants.TARGETS].values, dtype=torch.float32)\n",
    "  kl_score = calculate_kl_score(true_labels, pred_labels)\n",
    "  print(f\"KL Divergence Score on CV Data: {kl_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
