{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREPARATION_VOTE_METHOD = \"max_vote_window\" # \"max_vote_window\" or \"sum_and_normalize\". Decides how to aggregate the predictions of the overlapping windows\n",
    "EXISTING_CHECKPOINT_KAGGLE_DATASET_ID = \"hsm-models\" # set to the Kaggle dataset ID where the existing model checkpoints are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 19:43:11,224 :: root :: INFO :: Initialising Utils\n",
      "2025-11-12 19:43:11,740 :: root :: INFO :: Initialising Datasets\n",
      "2025-11-12 19:43:11,943 :: root :: INFO :: Initialising Models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping module tcn due to missing dependency: No module named 'pytorch_tcn'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/miniconda3/envs/aicomp/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/david/miniconda3/envs/aicomp/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "if bool(os.environ.get(\"KAGGLE_URL_BASE\", \"\")):\n",
    "  import sys\n",
    "  # running on kaggle\n",
    "  sys.path.insert(0, \"/kaggle/input/hsm-source-files\")\n",
    "else:\n",
    "  # running locally\n",
    "  sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "\n",
    "from src.utils.utils import get_raw_data_dir, get_submission_csv_path, get_models_save_path, set_seeds\n",
    "from src.utils.constants import Constants\n",
    "\n",
    "from src.datasets.cbramod_dataset import CBraModDataset\n",
    "from src.models.cbramod_model import CBraModModel\n",
    "\n",
    "set_seeds(Constants.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = get_raw_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfCFG:\n",
    "    data_path = DATA_PATH\n",
    "    n_splits = 5\n",
    "    batch_size = 64\n",
    "    num_workers = 8\n",
    "    model_dir = get_models_save_path(EXISTING_CHECKPOINT_KAGGLE_DATASET_ID) / \"cbramod\" / DATA_PREPARATION_VOTE_METHOD\n",
    "    dropout_prob = 0.1\n",
    "    window_size_seconds = 5\n",
    "    eeg_normalization = \"naive\"\n",
    "    apply_preprocessing = False\n",
    "\n",
    "InfCFG.model_paths = [os.path.join(InfCFG.model_dir, f'best_model_fold_{i}.pth') for i in range(InfCFG.n_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(device):\n",
    "  return CBraModModel(\n",
    "    pretrained_weights_path=None, # no need to load pretrained weights here as we will load full model weights later\n",
    "    classifier_type=\"all_patch_reps\",\n",
    "    num_of_classes=len(Constants.TARGETS),\n",
    "    dropout_prob=InfCFG.dropout_prob,\n",
    "    num_eeg_channels=len(Constants.EEG_FEATURES),\n",
    "    seq_len_seconds=InfCFG.window_size_seconds,\n",
    "    device=device,\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    test_df = pd.read_csv(os.path.join(InfCFG.data_path, 'test.csv'))\n",
    "    test_dataset = CBraModDataset(\n",
    "        test_df, \n",
    "        data_path=InfCFG.data_path, \n",
    "        window_size_seconds=InfCFG.window_size_seconds, \n",
    "        eeg_frequency=200,\n",
    "        mode='test',\n",
    "        normalization=InfCFG.eeg_normalization,\n",
    "        apply_preprocessing=InfCFG.apply_preprocessing\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=InfCFG.batch_size, shuffle=False, num_workers=InfCFG.num_workers\n",
    "    )\n",
    "\n",
    "    all_fold_predictions = []\n",
    "\n",
    "    for i, path in enumerate(InfCFG.model_paths):\n",
    "        print(f\"\\n========== Inferencing with Fold {i} Model ==========\")\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Model file not found: {path}. Skipping this fold.\")\n",
    "            continue\n",
    "            \n",
    "        model = create_model(device)\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        current_fold_preds = []\n",
    "        with torch.no_grad():\n",
    "            for eeg_windows in tqdm(test_loader, desc=f\"Predicting Fold {i}\"):\n",
    "                outputs = model(eeg_windows.to(device))\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                current_fold_preds.append(probs)\n",
    "        \n",
    "        all_fold_predictions.append(np.concatenate(current_fold_preds))\n",
    "\n",
    "    if not all_fold_predictions:\n",
    "        print(\"No models were found for inference. Aborting.\")\n",
    "        return\n",
    "\n",
    "    avg_predictions = np.mean(all_fold_predictions, axis=0)\n",
    "    submission = pd.DataFrame({\"eeg_id\": test_df[\"eeg_id\"]})\n",
    "    submission[Constants.TARGETS] = avg_predictions\n",
    "\n",
    "    submission.to_csv(get_submission_csv_path(), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Inferencing with Fold 0 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 0: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 1 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 1: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 2 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 2: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 3 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 3: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 4 Model ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Fold 4: 100%|██████████| 1/1 [00:00<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
