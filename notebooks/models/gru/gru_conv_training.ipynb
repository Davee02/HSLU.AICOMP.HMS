{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 08:49:54,762 :: root :: INFO :: Initialising Utils\n",
      "2025-11-14 08:49:55,444 :: root :: INFO :: Initialising Datasets\n",
      "2025-11-14 08:49:55,469 :: root :: INFO :: Initialising Models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaikotrede\u001b[0m (\u001b[33mhms-hslu-aicomp-hs25\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "from src.utils.k_folds_creator import KFoldCreator\n",
    "from src.utils.utils import get_models_save_path, set_seeds\n",
    "from src.utils.constants import Constants \n",
    "from src.datasets.eeg_dataset import EEGDataset\n",
    "from src.models.gru_convolution1d import GRUConvModel\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54816519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    data_path = '../../../data/'\n",
    "\n",
    "    model_name = 'GRUConv'\n",
    "    hidden_units = 256\n",
    "    num_layers = 2\n",
    "    target_size = 6 \n",
    "    \n",
    "    num_cnn_blocks = 3 \n",
    "    \n",
    "    sampling_rate = 200 # Hz\n",
    "    sequence_duration = 50 \n",
    "    downsample_factor = 1\n",
    "    \n",
    "    num_channels = 20 \n",
    "    \n",
    "    dropout = 0.4\n",
    "    batch_size = 64\n",
    "    num_workers = 8\n",
    "    epochs = 30\n",
    "    lr = 1e-4\n",
    "\n",
    "CFG.sequence_length = CFG.sequence_duration * CFG.sampling_rate \n",
    "\n",
    "set_seeds(CFG.seed)\n",
    "\n",
    "TARGETS = Constants.TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8fe4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(df, fold_id):\n",
    "    train_df = df[df['fold'] != fold_id].reset_index(drop=True)\n",
    "    valid_df = df[df['fold'] == fold_id].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = EEGDataset(df=train_df, data_path=CFG.data_path, mode='train', downsample_factor=CFG.downsample_factor)\n",
    "\n",
    "    valid_dataset = EEGDataset(df=valid_df, data_path=CFG.data_path, mode='valid', downsample_factor=CFG.downsample_factor)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=CFG.batch_size, shuffle=True,\n",
    "        num_workers=CFG.num_workers, pin_memory=True, drop_last=True, persistent_workers=True if CFG.num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=CFG.batch_size, shuffle=False,\n",
    "        num_workers=CFG.num_workers, pin_memory=True, drop_last=False, persistent_workers=True if CFG.num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fdb8515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data and creating folds...\n",
      "Train shape: (17089, 15)\n",
      "Targets ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
      "Folds created. Value counts per fold:\n",
      "fold\n",
      "0    4067\n",
      "1    3658\n",
      "2    3381\n",
      "4    3358\n",
      "3    2625\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def run_training(df, DATA_PREPARATION_VOTE_METHOD):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    all_oof = []\n",
    "    all_true = []\n",
    "    \n",
    "    fold_scores = []\n",
    "\n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"\\n========== FOLD {fold} ==========\")\n",
    "\n",
    "        experiment_group = f\"blocks_{CFG.num_cnn_blocks}\"\n",
    "        experiment_name = f\"{experiment_group}_fold{fold}\"\n",
    "\n",
    "        config = {\n",
    "            \"architecture\": CFG.model_name, \"hidden_units\": CFG.hidden_units, \n",
    "            \"num_layers\": CFG.num_layers,\n",
    "            \"fold\": fold, \"features\": \"raw_eeg\", \n",
    "            \"sequence_duration\": f\"{CFG.sequence_duration}s\",\n",
    "            \"optimizer\": \"AdamW\", \"learning_rate\": CFG.lr, \n",
    "            \"batch_size\": CFG.batch_size,\n",
    "            \"epochs\": CFG.epochs, \"seed\": CFG.seed, \n",
    "            \"Scheduler\": \"CosineAnnealingLR\",\n",
    "            \n",
    "            \"num_cnn_blocks\": CFG.num_cnn_blocks \n",
    "        }\n",
    "\n",
    "        wandb.init(\n",
    "            project=\"hms-aicomp-gru-conv\",\n",
    "            name=experiment_name,  \n",
    "            group=experiment_group,\n",
    "            tags=['gru-conv', f'fold{fold}', f'blocks_{CFG.num_cnn_blocks}'],\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        model = GRUConvModel(\n",
    "            input_size=CFG.num_channels,\n",
    "            hidden_size=CFG.hidden_units,\n",
    "            num_layers=CFG.num_layers,\n",
    "            num_classes=CFG.target_size,\n",
    "            \n",
    "            num_cnn_blocks=CFG.num_cnn_blocks,\n",
    "            dropout=CFG.dropout\n",
    "        )\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=1e-2)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.epochs)\n",
    "        loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
    "        train_loader, valid_loader = get_dataloaders(df, fold)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_path = None\n",
    "\n",
    "        for epoch in range(CFG.epochs):\n",
    "            print(f\"   --- Epoch {epoch+1}/{CFG.epochs} ---\")\n",
    "            \n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for signals, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "                signals, labels = signals.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(signals)\n",
    "                log_probs = F.log_softmax(outputs, dim=1)\n",
    "                loss = loss_fn(log_probs, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * signals.size(0)\n",
    "                wandb.log({\"train/loss\": loss.item()})\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for i, (signals, labels) in enumerate(tqdm(valid_loader, desc=\"Validation\")):\n",
    "                    signals, labels = signals.to(device), labels.to(device)\n",
    "                    outputs = model(signals)\n",
    "                    log_probs = F.log_softmax(outputs, dim=1)\n",
    "                    loss = loss_fn(log_probs, labels)\n",
    "                    valid_loss += loss.item() * signals.size(0)\n",
    "\n",
    "            valid_loss /= len(valid_loader.dataset)\n",
    "            \n",
    "            epoch_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"   Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Valid Loss = {valid_loss:.4f}, LR = {epoch_lr:.6f}\")\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1, \"train/epoch_loss\": train_loss, \"val/loss\": valid_loss,\n",
    "                \"val/kl_div\": valid_loss, \"train/epoch_lr\": epoch_lr\n",
    "            })\n",
    "\n",
    "            if valid_loss < best_val_loss:\n",
    "                best_val_loss = valid_loss\n",
    "                best_model_path = get_models_save_path() / \"GRUConvModel\" / DATA_PREPARATION_VOTE_METHOD / f'best_model_fold{fold}.pth'\n",
    "                best_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"   New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "            scheduler.step()\n",
    "        \n",
    "        fold_scores.append(best_val_loss)\n",
    "        wandb.summary['best_val_kl_div'] = best_val_loss\n",
    "\n",
    "        if best_model_path:\n",
    "            print(f\"Loading best model from fold {fold} to generate OOF predictions...\")\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "            model.eval()\n",
    "\n",
    "            fold_oof_preds = []\n",
    "            fold_oof_labels = []\n",
    "            with torch.no_grad():\n",
    "                for signals, labels in tqdm(valid_loader, desc=f\"Generating OOF for Fold {fold}\"):\n",
    "                    signals = signals.to(device)\n",
    "                    outputs = model(signals)\n",
    "                    probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                    fold_oof_preds.append(probs)\n",
    "                    fold_oof_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            all_oof.extend(np.concatenate(fold_oof_preds))\n",
    "            all_true.extend(np.concatenate(fold_oof_labels))\n",
    "\n",
    "            artifact = wandb.Artifact(f'model-fold{fold}', type='model')\n",
    "            artifact.add_file(best_model_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "            print(f\"\\nLogged artifact for fold {fold} with best validation loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            print(\"\\nNo best model was saved during training for this fold.\")\n",
    "        \n",
    "        wandb.finish()\n",
    "\n",
    "    print(\"\\nCalculating final OOF CV score from all collected predictions...\")\n",
    "    \n",
    "    valid_indices = df[df['fold'].isin(range(CFG.n_splits))].index\n",
    "    oof_df = pd.DataFrame(all_oof, index=valid_indices, columns=TARGETS)\n",
    "    true_df = pd.DataFrame(all_true, index=valid_indices, columns=TARGETS)\n",
    "    \n",
    "    oof_df = oof_df.sort_index()\n",
    "    true_df = true_df.sort_index()\n",
    "\n",
    "    oof_tensor = torch.tensor(oof_df.values, dtype=torch.float32)\n",
    "    true_tensor = torch.tensor(true_df.values, dtype=torch.float32)\n",
    "\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    overall_cv_score = kl_loss(torch.log(oof_tensor), true_tensor).item()\n",
    "\n",
    "    return overall_cv_score, fold_scores\n",
    "\n",
    "DATA_PREPARATION_VOTE_METHOD = \"sum_and_normalize\"\n",
    "\n",
    "print(\"Preparing data and creating folds...\")\n",
    "df = pd.read_csv(CFG.data_path + 'processed_data_max_vote_window.csv') \n",
    "\n",
    "label_map = {t: i for i, t in enumerate(TARGETS)}\n",
    "df['expert_consensus'] = df[TARGETS].idxmax(axis=1)\n",
    "\n",
    "print('Train shape:', df.shape)\n",
    "print('Targets', list(TARGETS))\n",
    "\n",
    "fold_creator = KFoldCreator(n_splits=CFG.n_splits, seed=CFG.seed)\n",
    "df = fold_creator.create_folds(df, stratify_col='expert_consensus', group_col='patient_id')\n",
    "\n",
    "print(\"Folds created. Value counts per fold:\")\n",
    "print(df['fold'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_to_run = [5]  \n",
    "\n",
    "experiment_results = {}\n",
    "\n",
    "print(f\"Starting systematic evaluation for num_cnn_blocks: {experiments_to_run}...\")\n",
    "\n",
    "for num_blocks in experiments_to_run:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"      STARTING EXPERIMENT: {num_blocks} CNN BLOCKS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    CFG.num_cnn_blocks = num_blocks\n",
    "    \n",
    "\n",
    "    overall_cv_score, all_fold_scores = run_training(df, DATA_PREPARATION_VOTE_METHOD)\n",
    "    \n",
    "    mean_fold_score = np.mean(all_fold_scores)\n",
    "    \n",
    "    experiment_results[num_blocks] = {\n",
    "        'overall_cv_score': overall_cv_score,\n",
    "        'mean_fold_score': mean_fold_score\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"      FINISHED EXPERIMENT: {num_blocks} CNN BLOCKS\")\n",
    "    print(f\"      OOF KL Score (all folds): {overall_cv_score:.4f}\")\n",
    "    print(f\"      Mean of fold scores: {mean_fold_score:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "for num_blocks, results in experiment_results.items():\n",
    "    print(f\"{num_blocks:<12} | {results['overall_cv_score']:<18.4f} | {results['mean_fold_score']:<18.4f}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"All experiments complete. Check wandb for detailed charts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef0533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF KL Score (calculated across all folds): 0.7971\n",
      "Mean of individual fold scores: 0.8011\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF KL Score (calculated across all folds): {overall_cv_score:.4f}\")\n",
    "print(f\"Mean of individual fold scores: {np.mean(all_fold_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
