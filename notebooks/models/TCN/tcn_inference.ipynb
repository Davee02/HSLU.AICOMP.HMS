{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e499a7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 12:23:29,226 :: root :: INFO :: Initialising Utils\n",
      "2025-10-22 12:23:29,979 :: root :: INFO :: Initialising Datasets\n",
      "2025-10-22 12:23:30,006 :: root :: INFO :: Initialising Models\n"
     ]
    }
   ],
   "source": [
    "DATA_PREPARATION_VOTE_METHOD = \"sum_and_normalize\" \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "\n",
    "if bool(os.environ.get(\"KAGGLE_URL_BASE\", \"\")):\n",
    "    tcn_source_path = '/kaggle/input/pytorch-tcn-library/pytorch_tcn-1.2.3/'\n",
    "    sys.path.append(tcn_source_path)\n",
    "from pytorch_tcn import TCN\n",
    "    \n",
    "if bool(os.environ.get(\"KAGGLE_URL_BASE\", \"\")):\n",
    "    sys.path.insert(0, \"/kaggle/input/hsm-source-files\")\n",
    "    \n",
    "else:\n",
    "    sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "\n",
    "from src.utils.utils import get_models_save_path, get_submission_csv_path, running_in_kaggle, get_raw_data_dir\n",
    "from src.utils.constants import Constants\n",
    "from src.datasets.eeg_dataset import EEGDataset\n",
    "from src.models.tcn import TCNModel \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa321941",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = get_raw_data_dir()\n",
    "\n",
    "class InfCFG:\n",
    "    model_name = 'TCN'\n",
    "    num_tcn_channels = [64, 128, 128, 256, 256, 512, 512, 512]\n",
    "    kernel_size = 21 \n",
    "    dropout = 0.35\n",
    "    \n",
    "    target_size = 6\n",
    "    num_channels = 20 \n",
    "    \n",
    "    data_path = DATA_PATH\n",
    "    n_splits = 5\n",
    "    \n",
    "    batch_size = 32\n",
    "    num_workers = 0\n",
    "    \n",
    "    downsample_factor = 3\n",
    "    \n",
    "    if running_in_kaggle():\n",
    "        MODEL_DIR = f'/kaggle/input/tcn-sum-votes/' \n",
    "    else:\n",
    "        MODEL_DIR = get_models_save_path() / \"TCNModel\" / DATA_PREPARATION_VOTE_METHOD\n",
    "\n",
    "InfCFG.model_paths = [os.path.join(InfCFG.MODEL_DIR, f'best_model_fold{i}.pth') for i in range(InfCFG.n_splits)]\n",
    "\n",
    "TARGETS = Constants.TARGETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6b3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_inference():\n",
    "    \"\"\"Executes the main inference loop.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    test_df = pd.read_csv(os.path.join(InfCFG.data_path, 'test.csv'))\n",
    "    \n",
    "    test_dataset = EEGDataset(df=test_df, data_path=InfCFG.data_path, mode='test', downsample_factor=InfCFG.downsample_factor)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=InfCFG.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=InfCFG.num_workers\n",
    "    )\n",
    "\n",
    "    all_fold_predictions = []\n",
    "\n",
    "    for i, path in enumerate(InfCFG.model_paths):\n",
    "        print(f\"\\n========== Inferencing with Fold {i} Model ==========\")\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Model file not found: {path}. Skipping this fold.\")\n",
    "            continue\n",
    "            \n",
    "        model = TCNModel(\n",
    "            num_inputs=InfCFG.num_channels,\n",
    "            num_outputs=InfCFG.target_size,\n",
    "            channel_sizes=InfCFG.num_tcn_channels,\n",
    "            kernel_size=InfCFG.kernel_size,\n",
    "            dropout=InfCFG.dropout,\n",
    "            causal=False,\n",
    "            use_skip_connections=True\n",
    "            \n",
    "        )\n",
    "        \n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        current_fold_preds = []\n",
    "        with torch.no_grad():\n",
    "            for signals in tqdm(test_loader, desc=f\"Predicting Fold {i}\"):\n",
    "                outputs = model(signals.to(device))\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                current_fold_preds.append(probs)\n",
    "        \n",
    "        all_fold_predictions.append(np.concatenate(current_fold_preds))\n",
    "\n",
    "    if not all_fold_predictions:\n",
    "        print(\"No models were found for inference. Aborting.\")\n",
    "        return\n",
    "\n",
    "    avg_predictions = np.mean(all_fold_predictions, axis=0)\n",
    "    \n",
    "    submission = pd.DataFrame({\"eeg_id\": test_df[\"eeg_id\"]})\n",
    "    submission[TARGETS] = avg_predictions\n",
    "    submission.to_csv(get_submission_csv_path(), index=False)\n",
    "    \n",
    "    print(\"\\nSubmission file created successfully!\")\n",
    "    print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c5e198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Inferencing with Fold 0 Model ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27be0c7627d4ea284fdcb3063c7d3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 1 Model ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1e9c48aed545bdbc59527b3f1d503b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 2 Model ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814b5adca470488ab86c8aff670992da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 3 Model ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aba250f94114824aa72f560e559a868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 3:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Inferencing with Fold 4 Model ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6410e3b0942c4e43801766ad7d67d136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 4:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file created successfully!\n",
      "       eeg_id  seizure_vote  lpd_vote  gpd_vote  lrda_vote  grda_vote  \\\n",
      "0  3911565283      0.042847  0.064649  0.003721   0.067294   0.036041   \n",
      "\n",
      "   other_vote  \n",
      "0    0.785448  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
