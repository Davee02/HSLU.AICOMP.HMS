{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dedbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ee914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing data and creating folds...\n",
      "Train shape: (17089, 12)\n",
      "Folds created. Value counts per fold:\n",
      "fold\n",
      "0    3741\n",
      "1    3703\n",
      "2    3527\n",
      "4    3081\n",
      "3    3037\n",
      "Name: count, dtype: int64\n",
      "\n",
      "===== Extracting CNN Features for Fold 0 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maiko/miniconda3/envs/aicomp/lib/python3.13/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a0a3dd0d89440095a4059c2f6ef049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting Fold 0 Features:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_854457/4054747813.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.HalfTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 232\u001b[39m\n\u001b[32m    227\u001b[39m     dataloader = DataLoader(\n\u001b[32m    228\u001b[39m         dataset, batch_size=CFG.fusion_batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=CFG.fusion_num_workers\n\u001b[32m    229\u001b[39m     )\n\u001b[32m    231\u001b[39m     \u001b[38;5;66;03m# 3. Extract and save\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[43mextract_and_save_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_feature_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# ### Run TCN Feature Extraction\u001b[39;00m\n\u001b[32m    236\u001b[39m \n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# TCN Feature Extraction\u001b[39;00m\n\u001b[32m    239\u001b[39m tcn_feature_path = CFG.feature_save_path / \u001b[33m\"\u001b[39m\u001b[33mtcn\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 183\u001b[39m, in \u001b[36mextract_and_save_features\u001b[39m\u001b[34m(model, dataloader, save_path, device, fold_id)\u001b[39m\n\u001b[32m    180\u001b[39m batch_data = batch_data.to(device)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast(enabled=(device.type == \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     features = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m all_features.append(features.cpu().numpy())\n\u001b[32m    186\u001b[39m all_labels.append(batch_labels.cpu().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aicomp/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aicomp/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 141\u001b[39m, in \u001b[36mCNNFeatureExtractor.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    138\u001b[39m x_3_channel = x_reshaped.repeat(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Now pass it through the feature extractor\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_3_channel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F.adaptive_avg_pool2d(features, (\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)).squeeze()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aicomp/lib/python3.13/site-packages/timm/models/efficientnet.py:312\u001b[39m, in \u001b[36mEfficientNet.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    311\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass through feature extraction layers.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_stem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn1(x)\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.grad_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aicomp/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aicomp/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aicomp/lib/python3.13/site-packages/timm/layers/conv2d_same.py:53\u001b[39m, in \u001b[36mConv2dSame.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv2d_same\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aicomp/lib/python3.13/site-packages/timm/layers/conv2d_same.py:28\u001b[39m, in \u001b[36mconv2d_same\u001b[39m\u001b[34m(x, weight, bias, stride, padding, dilation, groups)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconv2d_same\u001b[39m(\n\u001b[32m     19\u001b[39m         x,\n\u001b[32m     20\u001b[39m         weight: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m         groups: \u001b[38;5;28mint\u001b[39m = \u001b[32m1\u001b[39m,\n\u001b[32m     26\u001b[39m ):\n\u001b[32m     27\u001b[39m     x = pad_same(x, weight.shape[-\u001b[32m2\u001b[39m:], stride, dilation)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (torch.cuda.HalfTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Multimodal CNN + TCN with Feature Fusion\n",
    "# \n",
    "# This notebook implements a multimodal approach by combining a Convolutional Neural Network (CNN) trained on EEG spectrograms and a Temporal Convolutional Network (TCN) trained on raw EEG signals.\n",
    "# \n",
    "# Due to limited computing resources, we'll use a three-stage approach:\n",
    "# 1.  **Extract Features**: Load the trained models for each fold and extract feature embeddings from an intermediate layer. Save these features to disk.\n",
    "# 2.  **Create Fusion Dataset**: Build a new PyTorch Dataset that loads the pre-saved CNN and TCN features and concatenates them.\n",
    "# 3.  **Train Classifier Head**: Train a simple MLP classifier head on the fused features.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add project root to system path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "from src.datasets.eeg_dataset import EEGDataset\n",
    "from src.datasets.multi_spectrogram import MultiSpectrogramDataset\n",
    "from src.models.base_cnn import BaseCNN\n",
    "from src.models.tcn import TCNModel\n",
    "from src.utils.constants import Constants\n",
    "from src.utils.k_folds_creator import KFoldCreator\n",
    "from src.utils.utils import get_models_save_path\n",
    "\n",
    "# Log in to W&B\n",
    "wandb.login()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Configuration\n",
    "# \n",
    "# We'll define a configuration class `CFG` to hold all hyperparameters and paths for both models and the final classifier head.\n",
    "\n",
    "# %%\n",
    "class CFG:\n",
    "    # General\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    data_path = '../../../data/'\n",
    "    \n",
    "    # Paths\n",
    "    models_save_path = get_models_save_path()\n",
    "    feature_save_path = Path('../../../data/features/')\n",
    "    DATA_PREPARATION_VOTE_METHOD = \"sum_and_normalize\"\n",
    "    \n",
    "    # CNN Config\n",
    "    cnn_model_name = 'tf_efficientnet_b0_ns'\n",
    "    cnn_in_channels = 8\n",
    "    cnn_img_size = (128, 256)\n",
    "    cnn_eeg_spec_path = '../../../data/custom_eegs/cwt'\n",
    "    \n",
    "    # TCN Config\n",
    "    tcn_model_name = 'TCN'\n",
    "    tcn_num_channels = 20\n",
    "    tcn_num_tcn_channels = [64, 128, 128, 256, 256, 512, 512, 512]\n",
    "    tcn_kernel_size = 21\n",
    "    tcn_dropout = 0.35\n",
    "    tcn_sequence_duration = 50\n",
    "    tcn_original_sampling_rate = 200\n",
    "    tcn_downsample_factor = 3\n",
    "    tcn_sampling_rate = tcn_original_sampling_rate // tcn_downsample_factor\n",
    "    tcn_sequence_length = tcn_sequence_duration * tcn_sampling_rate\n",
    "    \n",
    "    # Classifier Head Config\n",
    "    fusion_batch_size = 64\n",
    "    fusion_num_workers = 4\n",
    "    fusion_epochs = 10\n",
    "    fusion_lr = 5e-4\n",
    "    fusion_hidden_size = 512\n",
    "    target_size = 6\n",
    "\n",
    "# Create directory for saving features\n",
    "CFG.feature_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "TARGETS = Constants.TARGETS\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Data Preparation\n",
    "# \n",
    "# We'll load the processed data and create the same stratified group k-folds that were used to train the individual models. This ensures consistency.\n",
    "\n",
    "# %%\n",
    "print(\"Preparing data and creating folds...\")\n",
    "df = pd.read_csv(CFG.data_path + 'processed_data_sum_votes_window.csv')\n",
    "df['expert_consensus'] = df[TARGETS].idxmax(axis=1)\n",
    "\n",
    "print('Train shape:', df.shape)\n",
    "\n",
    "fold_creator = KFoldCreator(n_splits=CFG.n_splits, seed=CFG.seed)\n",
    "df = fold_creator.create_folds(df, stratify_col='expert_consensus', group_col='patient_id')\n",
    "\n",
    "print(\"Folds created. Value counts per fold:\")\n",
    "print(df['fold'].value_counts())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Stage 1: Feature Extraction\n",
    "# \n",
    "# ### Modifying Models for Feature Extraction\n",
    "# We need to adapt the `BaseCNN` and `TCNModel` to return feature vectors instead of classification logits. We'll create new wrapper classes for this purpose.\n",
    "\n",
    "# %%\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_name, pretrained=True, in_channels=4, num_classes=6):\n",
    "        super().__init__()\n",
    "        # Load the original model structure\n",
    "        original_model = BaseCNN(model_name, pretrained, in_channels, num_classes)\n",
    "        # We use the feature extractor part of the timm model\n",
    "        self.feature_extractor = original_model.model.forward_features\n",
    "        # Keep the forward logic from BaseCNN\n",
    "        self.forward_logic = original_model.forward\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # We need to replicate the input processing from the original BaseCNN\n",
    "        channels = torch.split(x, 1, dim=1)\n",
    "        x_reshaped = torch.cat(channels, dim=2)\n",
    "        x_3_channel = x_reshaped.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        # Now pass it through the feature extractor\n",
    "        features = self.feature_extractor(x_3_channel)\n",
    "        \n",
    "        # Global average pooling\n",
    "        return F.adaptive_avg_pool2d(features, (1, 1)).squeeze()\n",
    "\n",
    "\n",
    "class TCNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_inputs, channel_sizes, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        self.tcn = TCN(\n",
    "            num_inputs=num_inputs,\n",
    "            num_channels=channel_sizes,\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,\n",
    "            causal=False,\n",
    "            use_skip_connections=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        tcn_output = self.tcn(x)\n",
    "        # Return the output of the last time step\n",
    "        return tcn_output[:, :, -1]\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Feature Extraction Loop\n",
    "# \n",
    "# This function will iterate through a dataloader, pass the data through a feature extractor model, and save the resulting features, labels, and `eeg_id`s.\n",
    "\n",
    "# %%\n",
    "def extract_and_save_features(model, dataloader, save_path, device, fold_id):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in tqdm(dataloader, desc=f\"Extracting Fold {fold_id} Features\"):\n",
    "            batch_data = batch_data.to(device)\n",
    "            \n",
    "            # The autocast context manager is now removed for inference\n",
    "            features = model(batch_data)\n",
    "                \n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(batch_labels.cpu().numpy())\n",
    "            \n",
    "    # Concatenate all batches\n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    # Save to disk\n",
    "    np.save(save_path / f'features_fold_{fold_id}.npy', all_features)\n",
    "    np.save(save_path / f'labels_fold_{fold_id}.npy', all_labels)\n",
    "    print(f\"Saved features for fold {fold_id} to {save_path}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Run CNN Feature Extraction\n",
    "\n",
    "# %%\n",
    "# CNN Feature Extraction\n",
    "cnn_feature_path = CFG.feature_save_path / \"cnn\"\n",
    "cnn_feature_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"\\n===== Extracting CNN Features for Fold {fold} =====\")\n",
    "    \n",
    "    # Check if features already exist\n",
    "    if (cnn_feature_path / f'features_fold_{fold}.npy').exists():\n",
    "        print(f\"CNN features for fold {fold} already exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 1. Load trained model weights\n",
    "    cnn_extractor = CNNFeatureExtractor(CFG.cnn_model_name, pretrained=False)\n",
    "    model_path = CFG.models_save_path / \"MultiSpectCNN\" / CFG.DATA_PREPARATION_VOTE_METHOD / f'best_model_fold{fold}.pth'\n",
    "    \n",
    "    # We need to load the state dict from the original BaseCNN into our extractor\n",
    "    original_state_dict = torch.load(model_path, map_location=DEVICE)\n",
    "    # The extractor's layers are a subset of the original model's layers\n",
    "    cnn_extractor.load_state_dict(original_state_dict, strict=False)\n",
    "\n",
    "    # 2. Create dataset for the entire fold (train + valid)\n",
    "    fold_df = df.copy() # Use the full dataframe\n",
    "    dataset = MultiSpectrogramDataset(\n",
    "        fold_df, TARGETS, CFG.data_path, CFG.cnn_img_size, CFG.cnn_eeg_spec_path, mode='train'\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=CFG.fusion_batch_size, shuffle=False, num_workers=CFG.fusion_num_workers\n",
    "    )\n",
    "    \n",
    "    # 3. Extract and save\n",
    "    extract_and_save_features(cnn_extractor, dataloader, cnn_feature_path, DEVICE, fold)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Run TCN Feature Extraction\n",
    "\n",
    "# %%\n",
    "# TCN Feature Extraction\n",
    "tcn_feature_path = CFG.feature_save_path / \"tcn\"\n",
    "tcn_feature_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for fold in range(CFG.n_splits):\n",
    "    print(f\"\\n===== Extracting TCN Features for Fold {fold} =====\")\n",
    "\n",
    "    if (tcn_feature_path / f'features_fold_{fold}.npy').exists():\n",
    "        print(f\"TCN features for fold {fold} already exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 1. Load trained model weights\n",
    "    tcn_extractor = TCNFeatureExtractor(\n",
    "        num_inputs=CFG.tcn_num_channels,\n",
    "        channel_sizes=CFG.tcn_num_tcn_channels,\n",
    "        kernel_size=CFG.tcn_kernel_size,\n",
    "        dropout=CFG.tcn_dropout,\n",
    "    )\n",
    "    model_path = CFG.models_save_path / \"TCNModel\" / CFG.DATA_PREPARATION_VOTE_METHOD / f'best_model_fold{fold}.pth'\n",
    "    original_state_dict = torch.load(model_path, map_location=DEVICE)\n",
    "    tcn_extractor.load_state_dict(original_state_dict, strict=False)\n",
    "    \n",
    "    # 2. Create dataset for the entire fold\n",
    "    fold_df = df.copy()\n",
    "    dataset = EEGDataset(\n",
    "        df=fold_df, data_path=CFG.data_path, mode='train',\n",
    "        downsample_factor=CFG.tcn_downsample_factor\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=CFG.fusion_batch_size, shuffle=False, num_workers=CFG.fusion_num_workers\n",
    "    )\n",
    "    \n",
    "    # 3. Extract and save\n",
    "    extract_and_save_features(tcn_extractor, dataloader, tcn_feature_path, DEVICE, fold)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Stage 2: Feature Fusion Dataset\n",
    "# \n",
    "# Now we create a `Dataset` that loads the features we just saved. This will be very fast as it only involves reading NumPy arrays from disk.\n",
    "\n",
    "# %%\n",
    "class FusedFeatureDataset(Dataset):\n",
    "    def __init__(self, df, fold_id, cnn_feature_path, tcn_feature_path, indices):\n",
    "        self.indices = indices\n",
    "        \n",
    "        # Load all features and labels for the given fold into memory\n",
    "        self.cnn_features = np.load(cnn_feature_path / f'features_fold_{fold_id}.npy')\n",
    "        self.tcn_features = np.load(tcn_feature_path / f'features_fold_{fold_id}.npy')\n",
    "        self.labels = np.load(cnn_feature_path / f'labels_fold_{fold_id}.npy') # Labels are the same\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original dataframe index\n",
    "        original_idx = self.indices[idx]\n",
    "        \n",
    "        cnn_feat = torch.tensor(self.cnn_features[original_idx], dtype=torch.float32)\n",
    "        tcn_feat = torch.tensor(self.tcn_features[original_idx], dtype=torch.float32)\n",
    "        \n",
    "        # Concatenate features\n",
    "        fused_features = torch.cat([cnn_feat, tcn_feat], dim=0)\n",
    "        \n",
    "        label = torch.tensor(self.labels[original_idx], dtype=torch.float32)\n",
    "        \n",
    "        return fused_features, label\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Stage 3: Classifier Head and Training Loop\n",
    "# \n",
    "# ### Define the Classifier Head\n",
    "# A simple MLP will serve as our classifier head. It takes the concatenated feature vector as input.\n",
    "\n",
    "# %%\n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Training and Evaluation Loop\n",
    "\n",
    "# %%\n",
    "def run_fusion_training(df, cnn_feature_path, tcn_feature_path):\n",
    "    all_oof_preds = []\n",
    "    all_oof_labels = []\n",
    "\n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"\\n========== FOLD {fold} ==========\")\n",
    "        \n",
    "        # Determine input size for the classifier head\n",
    "        cnn_feat_sample = np.load(cnn_feature_path / f'features_fold_{fold}.npy', mmap_mode='r')\n",
    "        tcn_feat_sample = np.load(tcn_feature_path / f'features_fold_{fold}.npy', mmap_mode='r')\n",
    "        input_size = cnn_feat_sample.shape[1] + tcn_feat_sample.shape[1]\n",
    "        print(f\"Input feature size: {input_size} (CNN: {cnn_feat_sample.shape[1]}, TCN: {tcn_feat_sample.shape[1]})\")\n",
    "\n",
    "        # Get train and validation indices based on folds\n",
    "        train_indices = df[df['fold'] != fold].index.values\n",
    "        valid_indices = df[df['fold'] == fold].index.values\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = FusedFeatureDataset(df, fold, cnn_feature_path, tcn_feature_path, train_indices)\n",
    "        valid_dataset = FusedFeatureDataset(df, fold, cnn_feature_path, tcn_feature_path, valid_indices)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=CFG.fusion_batch_size, shuffle=True, \n",
    "            num_workers=CFG.fusion_num_workers, drop_last=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset, batch_size=CFG.fusion_batch_size, shuffle=False, \n",
    "            num_workers=CFG.fusion_num_workers\n",
    "        )\n",
    "\n",
    "        model = FusionClassifier(input_size, CFG.fusion_hidden_size, CFG.target_size).to(DEVICE)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.fusion_lr)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.fusion_epochs)\n",
    "        loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(CFG.fusion_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(features)\n",
    "                log_probs = F.log_softmax(outputs, dim=1)\n",
    "                loss = loss_fn(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for features, labels in valid_loader:\n",
    "                    features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "                    outputs = model(features)\n",
    "                    log_probs = F.log_softmax(outputs, dim=1)\n",
    "                    loss = loss_fn(log_probs, labels)\n",
    "                    valid_loss += loss.item()\n",
    "            \n",
    "            valid_loss /= len(valid_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Valid Loss = {valid_loss:.4f}\")\n",
    "\n",
    "            if valid_loss < best_val_loss:\n",
    "                best_val_loss = valid_loss\n",
    "                torch.save(model.state_dict(), f'fusion_head_best_fold_{fold}.pth')\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "        # OOF Predictions\n",
    "        model.load_state_dict(torch.load(f'fusion_head_best_fold_{fold}.pth'))\n",
    "        model.eval()\n",
    "        fold_preds = []\n",
    "        fold_labels = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels in valid_loader:\n",
    "                features = features.to(DEVICE)\n",
    "                outputs = model(features)\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                fold_preds.append(probs)\n",
    "                fold_labels.append(labels.numpy())\n",
    "        \n",
    "        all_oof_preds.append(np.concatenate(fold_preds))\n",
    "        all_oof_labels.append(np.concatenate(fold_labels))\n",
    "\n",
    "    # Calculate final OOF score\n",
    "    final_oof_preds = np.concatenate(all_oof_preds)\n",
    "    final_oof_labels = np.concatenate(all_oof_labels)\n",
    "    \n",
    "    kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "    score = kl_loss(torch.log(torch.tensor(final_oof_preds)), torch.tensor(final_oof_labels)).item()\n",
    "    \n",
    "    print(f\"\\nOverall OOF KL Score: {score:.4f}\")\n",
    "    return score\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Run the Final Training\n",
    "\n",
    "# %%\n",
    "if __name__ == '__main__':\n",
    "    overall_oof_score = run_fusion_training(df, cnn_feature_path, tcn_feature_path)\n",
    "\n",
    "# %%\n",
    "print(f\"Final OOF KL Score from Multimodal Fusion: {overall_oof_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee150607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
