{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Support Vector Machine with Mean Spectrogram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREPARATION_VOTE_METHOD = \"sum_and_normalize\" # \"max_vote_window\" or \"sum_and_normalize\". Decides how to aggregate the predictions of the overlapping windows\n",
    "SKIP_TRAIN = False # If True, skips the training phase and only runs evaluation with existing checkpoints\n",
    "EXISTING_CHECKPOINT_KAGGLE_DATASET_ID = \"hsm-models\" # set to None if you want to train a new model on Kaggle. Else, set to the Kaggle dataset ID where the existing model checkpoints are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 17:14:19,778 :: root :: INFO :: Initialising Utils\n",
      "2025-10-13 17:14:19,781 :: root :: INFO :: Initialising Datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "import pathlib\n",
    "\n",
    "if bool(os.environ.get(\"KAGGLE_URL_BASE\", \"\")):\n",
    "  import sys\n",
    "  # running on kaggle\n",
    "  sys.path.insert(0, \"/kaggle/input/hsm-source-files\")\n",
    "else:\n",
    "  # running locally\n",
    "  sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "from src.utils.utils import get_raw_data_dir, get_processed_data_dir, get_submission_csv_path, set_seeds, get_models_save_path, running_in_kaggle\n",
    "from src.utils.constants import Constants\n",
    "from src.datasets.eeg_processor import EEGDataProcessor\n",
    "from src.utils.k_folds_creator import KFoldCreator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIP_TRAIN=False\n"
     ]
    }
   ],
   "source": [
    "# never train on kaggle, only evaluate\n",
    "SKIP_TRAIN = SKIP_TRAIN or running_in_kaggle()\n",
    "print(f\"SKIP_TRAIN={SKIP_TRAIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor initialized.\n",
      "Raw data path: '/home/david/git/aicomp/data'\n",
      "Processed data path: '/home/david/git/aicomp/data/processed'\n",
      "==================================================\n",
      "Starting EEG Data Processing Pipeline\n",
      "==================================================\n",
      "Skipping NumPy file creation as requested.\n",
      "Using 'sum_and_normalize' vote aggregation strategy with spectrogram info.\n",
      "\n",
      "Processed train data saved to '/home/david/git/aicomp/data/processed/train_processed.csv'.\n",
      "Shape of the final dataframe: (17089, 12)\n",
      "\n",
      "Pipeline finished successfully!\n",
      "==================================================\n",
      "Using models save path: /home/david/git/aicomp/models/svm/spectrogram_means/sum_and_normalize\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = get_raw_data_dir()\n",
    "\n",
    "processor = EEGDataProcessor(raw_data_path=DATA_PATH, processed_data_path=get_processed_data_dir())\n",
    "train_df = processor.process_data(vote_method=DATA_PREPARATION_VOTE_METHOD, skip_npy=True)\n",
    "\n",
    "test_df = pd.read_csv(DATA_PATH / \"test.csv\")\n",
    "\n",
    "kl_score = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "\n",
    "models_save_path = get_models_save_path(EXISTING_CHECKPOINT_KAGGLE_DATASET_ID) / \"svm\" / \"spectrogram_means\" / DATA_PREPARATION_VOTE_METHOD\n",
    "models_save_path.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Using models save path:\", models_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spectrogram Files into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram_content(spectrogram_file: pathlib.Path):\n",
    "  spectrogram_id = int(spectrogram_file.stem.split(\"_\")[-1])\n",
    "  content = pd.read_parquet(spectrogram_file)\n",
    "  content = content.drop(columns=[\"time\"]).values\n",
    "  return spectrogram_id, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11138 train spectrogram files to load into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11138/11138 [06:46<00:00, 27.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all train spectrograms into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_TRAIN:\n",
    "  spectrograms_dir = DATA_PATH / \"train_spectrograms\"\n",
    "  spectrogram_files = list(spectrograms_dir.glob(\"*.parquet\"))\n",
    "  print(f\"Found {len(spectrogram_files)} train spectrogram files to load into memory\")\n",
    "\n",
    "  spectrograms = {}\n",
    "  for file in tqdm(spectrogram_files):\n",
    "    spectrogram_id, content = get_spectrogram_content(file)\n",
    "    spectrograms[spectrogram_id] = content\n",
    "\n",
    "  gc.collect()\n",
    "  print(\"Loaded all train spectrograms into memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We use only use the mean of all frequencies (100 per chain => 400 in total) as features.\n",
    "We take the middle 10 minutes of all spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQUENCY_COUNT = 400 # each spectrogram has 400 frequency bins\n",
    "\n",
    "FEATURES = [f\"spec_mean_freq_{x}\" for x in range(FREQUENCY_COUNT)]\n",
    "\n",
    "def extract_spectrogram_features(ten_minute_window):\n",
    "  average_frequencies = ten_minute_window.mean(axis=0) # average over 300 rows (10 minutes)\n",
    "  return average_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17089/17089 [00:02<00:00, 5847.91it/s]\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_TRAIN:\n",
    "  data = np.zeros((len(train_df), len(FEATURES)))\n",
    "\n",
    "  def extract_train_spectrogram_features(row, all_spectrograms):\n",
    "    spectrogram_id = int(row[\"spectrogram_id\"])\n",
    "    middle_offset = (row[\"min_offset\"] + row[\"max_offset\"]) // 2 # this the middle between the least spectrogram offset and greatest spectogram offset\n",
    "    row_index = int(middle_offset // 2) # each spectrogram row corresponds to 2s, so we divide by 2 to get the row index\n",
    "    window = np.array(all_spectrograms[spectrogram_id][row_index:row_index+300,:])\n",
    "    average_frequencies = extract_spectrogram_features(window)\n",
    "    return average_frequencies\n",
    "\n",
    "  for i in tqdm(range(len(train_df)), total=len(train_df)):\n",
    "    row = train_df.iloc[i]\n",
    "    average_features = extract_train_spectrogram_features(row, spectrograms)\n",
    "    data[i,:] = average_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_TRAIN:\n",
    "  train_df[FEATURES] = data\n",
    "\n",
    "  train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "targets_dict = {\"Seizure\":0, \"LPD\":1, \"GPD\":2, \"LRDA\":3, \"GRDA\":4, \"Other\":5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_TRAIN:\n",
    "    fold_creator = KFoldCreator(n_splits=N_SPLITS, seed=Constants.SEED)\n",
    "    train_folds_df = fold_creator.create_folds(\n",
    "        df=train_df, stratify_col=\"expert_consensus\", group_col=\"patient_id\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_with_mean(X):\n",
    "    col_means = np.nanmean(X, axis=0)\n",
    "    X = np.nan_to_num(X, nan=col_means)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "FOLD 0\n",
      "Train size: 13755, Valid size: 3334\n",
      "==============================\n",
      "Training SVM...\n",
      "Predicting on validation set...\n",
      "========================================\n",
      "FOLD 1\n",
      "Train size: 13151, Valid size: 3938\n",
      "==============================\n",
      "Training SVM...\n",
      "Predicting on validation set...\n",
      "========================================\n",
      "FOLD 2\n",
      "Train size: 13422, Valid size: 3667\n",
      "==============================\n",
      "Training SVM...\n",
      "Predicting on validation set...\n",
      "========================================\n",
      "FOLD 3\n",
      "Train size: 14356, Valid size: 2733\n",
      "==============================\n",
      "Training SVM...\n",
      "Predicting on validation set...\n",
      "========================================\n",
      "FOLD 4\n",
      "Train size: 13672, Valid size: 3417\n",
      "==============================\n",
      "Training SVM...\n",
      "Predicting on validation set...\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_TRAIN:\n",
    "    all_oof = []\n",
    "    all_true = []\n",
    "\n",
    "    for fold in range(N_SPLITS):\n",
    "        fold_train_df = train_folds_df[train_folds_df[\"fold\"] != fold].reset_index(drop=True)\n",
    "        fold_valid_df = train_folds_df[train_folds_df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"FOLD {fold}\")\n",
    "        print(f\"Train size: {len(fold_train_df)}, Valid size: {len(fold_valid_df)}\")\n",
    "        print(\"=\" * 30)\n",
    "\n",
    "        X_train = fold_train_df[FEATURES].values\n",
    "        y_train = fold_train_df[\"expert_consensus\"].map(targets_dict).values\n",
    "\n",
    "        X_valid = fold_valid_df[FEATURES].values\n",
    "        y_valid = fold_valid_df[\"expert_consensus\"].map(targets_dict).values\n",
    "\n",
    "        X_train = fill_nan_with_mean(X_train)\n",
    "        X_valid = fill_nan_with_mean(X_valid)\n",
    "\n",
    "        # scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "        # train SVM with probability estimates\n",
    "        model = SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            probability=True,\n",
    "            random_state=Constants.SEED,\n",
    "            verbose=False,\n",
    "            cache_size=1000 # 1 GB\n",
    "        )\n",
    "        \n",
    "        print(\"Training SVM...\")\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        joblib.dump(model, models_save_path / f\"fold_{fold}_model.pkl\")\n",
    "        joblib.dump(scaler, models_save_path / f\"fold_{fold}_scaler.pkl\")\n",
    "        \n",
    "        print(\"Predicting on validation set...\")\n",
    "        oof = model.predict_proba(X_valid_scaled)\n",
    "        all_oof.extend(oof)\n",
    "\n",
    "        all_true.extend(fold_valid_df[Constants.TARGETS].values)\n",
    "\n",
    "        del X_train, y_train, X_valid, y_valid, X_train_scaled, X_valid_scaled, oof\n",
    "        gc.collect()\n",
    "\n",
    "    all_oof = np.array(all_oof)\n",
    "    all_true = np.array(all_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF KL Score: 1.2697938680648804\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_TRAIN:\n",
    "  all_oof_tensor = torch.tensor(all_oof, dtype=torch.float32)\n",
    "  all_true_tensor = torch.tensor(all_true, dtype=torch.float32)\n",
    "\n",
    "  kl_score = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "  score = kl_score(all_oof_tensor.log(), all_true_tensor).item()\n",
    "\n",
    "  print(f\"OOF KL Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer on Test and create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 test spectrogram files to load into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all test spectrograms into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_spectrograms_dir = DATA_PATH / \"test_spectrograms\"\n",
    "test_spectrogram_files = list(test_spectrograms_dir.glob(\"*.parquet\"))\n",
    "print(f\"Found {len(test_spectrogram_files)} test spectrogram files to load into memory\")\n",
    "\n",
    "test_spectrograms = {}\n",
    "for file in tqdm(test_spectrogram_files):\n",
    "  spectrogram_id, content = get_spectrogram_content(file)\n",
    "  test_spectrograms[spectrogram_id] = content\n",
    "\n",
    "gc.collect()\n",
    "print(\"Loaded all test spectrograms into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1612.57it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = np.zeros((len(test_df), len(FEATURES)))\n",
    "\n",
    "def extract_test_spectrogram_features(row, all_spectrograms):\n",
    "  # this differs from train because all test spectrograms are exactly 10 minutes long, so we don't need to extract the center window\n",
    "  spectrogram_id = int(row[\"spectrogram_id\"])\n",
    "  content = np.array(all_spectrograms[spectrogram_id][:])\n",
    "  average_frequencies = extract_spectrogram_features(content)\n",
    "  return average_frequencies\n",
    "\n",
    "for i in tqdm(range(len(test_df)), total=len(test_df)):\n",
    "  row = test_df.iloc[i]\n",
    "  average_features = extract_test_spectrogram_features(row, test_spectrograms)\n",
    "  test_data[i,:] = average_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>spec_mean_freq_0</th>\n",
       "      <th>spec_mean_freq_1</th>\n",
       "      <th>spec_mean_freq_2</th>\n",
       "      <th>spec_mean_freq_3</th>\n",
       "      <th>spec_mean_freq_4</th>\n",
       "      <th>spec_mean_freq_5</th>\n",
       "      <th>spec_mean_freq_6</th>\n",
       "      <th>...</th>\n",
       "      <th>spec_mean_freq_390</th>\n",
       "      <th>spec_mean_freq_391</th>\n",
       "      <th>spec_mean_freq_392</th>\n",
       "      <th>spec_mean_freq_393</th>\n",
       "      <th>spec_mean_freq_394</th>\n",
       "      <th>spec_mean_freq_395</th>\n",
       "      <th>spec_mean_freq_396</th>\n",
       "      <th>spec_mean_freq_397</th>\n",
       "      <th>spec_mean_freq_398</th>\n",
       "      <th>spec_mean_freq_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>853520</td>\n",
       "      <td>3911565283</td>\n",
       "      <td>6885</td>\n",
       "      <td>16.864132</td>\n",
       "      <td>19.120565</td>\n",
       "      <td>18.342468</td>\n",
       "      <td>13.408634</td>\n",
       "      <td>8.0575</td>\n",
       "      <td>4.890133</td>\n",
       "      <td>3.460633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088567</td>\n",
       "      <td>0.086333</td>\n",
       "      <td>0.083633</td>\n",
       "      <td>0.084067</td>\n",
       "      <td>0.081933</td>\n",
       "      <td>0.082867</td>\n",
       "      <td>0.084267</td>\n",
       "      <td>0.082633</td>\n",
       "      <td>0.083967</td>\n",
       "      <td>0.081533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 403 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   spectrogram_id      eeg_id  patient_id  spec_mean_freq_0  spec_mean_freq_1  \\\n",
       "0          853520  3911565283        6885         16.864132         19.120565   \n",
       "\n",
       "   spec_mean_freq_2  spec_mean_freq_3  spec_mean_freq_4  spec_mean_freq_5  \\\n",
       "0         18.342468         13.408634            8.0575          4.890133   \n",
       "\n",
       "   spec_mean_freq_6  ...  spec_mean_freq_390  spec_mean_freq_391  \\\n",
       "0          3.460633  ...            0.088567            0.086333   \n",
       "\n",
       "   spec_mean_freq_392  spec_mean_freq_393  spec_mean_freq_394  \\\n",
       "0            0.083633            0.084067            0.081933   \n",
       "\n",
       "   spec_mean_freq_395  spec_mean_freq_396  spec_mean_freq_397  \\\n",
       "0            0.082867            0.084267            0.082633   \n",
       "\n",
       "   spec_mean_freq_398  spec_mean_freq_399  \n",
       "0            0.083967            0.081533  \n",
       "\n",
       "[1 rows x 403 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[FEATURES] = test_data\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Predicting fold 0\n",
      "========================================\n",
      "========================================\n",
      "Predicting fold 1\n",
      "========================================\n",
      "========================================\n",
      "Predicting fold 2\n",
      "========================================\n",
      "========================================\n",
      "Predicting fold 3\n",
      "========================================\n",
      "========================================\n",
      "Predicting fold 4\n",
      "========================================\n",
      "Test predictions shape: (1, 6)\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "  print(\"=\" * 40)\n",
    "  print(f\"Predicting fold {fold}\")\n",
    "  print(\"=\" * 40)\n",
    "\n",
    "  X_test = test_df[FEATURES].values\n",
    "  X_test = fill_nan_with_mean(X_test)\n",
    "\n",
    "  scaler = joblib.load(models_save_path / f\"fold_{fold}_scaler.pkl\")\n",
    "  model = joblib.load(models_save_path / f\"fold_{fold}_model.pkl\")\n",
    "\n",
    "  X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "  preds = model.predict_proba(X_test_scaled)\n",
    "  test_preds.append(preds)\n",
    "\n",
    "test_preds = np.mean(test_preds, axis=0)\n",
    "print(f\"Test predictions shape: {test_preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: all predictions should sum to 1\n",
    "assert np.allclose(test_preds.sum(axis=1), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"eeg_id\": test_df[\"eeg_id\"]})\n",
    "submission[Constants.TARGETS] = test_preds\n",
    "\n",
    "submission.to_csv(get_submission_csv_path(), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
