{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine with Crafted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREPARATION_VOTE_METHOD = \"max_vote_window\" # \"max_vote_window\" or \"sum_and_normalize\". Decides how to aggregate the predictions of the overlapping windows\n",
    "SKIP_TRAIN = False # If True, skips the training phase and only runs evaluation with existing checkpoints\n",
    "EXISTING_CHECKPOINT_KAGGLE_DATASET_ID = \"hsm-models\" # set to None if you want to train a new model on Kaggle. Else, set to the Kaggle dataset ID where the existing model checkpoints are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 14:56:47,328 :: root :: INFO :: Initialising Utils\n",
      "2025-10-10 14:56:47,330 :: root :: INFO :: Initialising Datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "import pathlib\n",
    "\n",
    "if bool(os.environ.get(\"KAGGLE_URL_BASE\", \"\")):\n",
    "  import sys\n",
    "  # running on kaggle\n",
    "  sys.path.insert(0, \"/kaggle/input/hsm-source-files\")\n",
    "else:\n",
    "  # running locally\n",
    "  sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\")))\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "from src.utils.utils import get_raw_data_dir, get_processed_data_dir, get_submission_csv_path, set_seeds, get_models_save_path, running_in_kaggle, fill_nan_with_mean\n",
    "from src.utils.constants import Constants\n",
    "from src.datasets.eeg_processor import EEGDataProcessor\n",
    "from src.utils.k_folds_creator import KFoldCreator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# never train on kaggle, only evaluate\n",
    "SKIP_TRAIN = SKIP_TRAIN or running_in_kaggle()\n",
    "print(f\"SKIP_TRAIN = {SKIP_TRAIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor initialized.\n",
      "Raw data path: '/home/david/git/aicomp/data'\n",
      "Processed data path: '/home/david/git/aicomp/data/processed'\n",
      "==================================================\n",
      "Starting EEG Data Processing Pipeline\n",
      "==================================================\n",
      "Skipping NumPy file creation as requested.\n",
      "Using 'max_vote_window' vote aggregation strategy.\n",
      "\n",
      "Processed train data saved to '/home/david/git/aicomp/data/processed/train_processed.csv'.\n",
      "Shape of the final dataframe: (17089, 12)\n",
      "\n",
      "Pipeline finished successfully!\n",
      "==================================================\n",
      "Using models save path: /home/david/git/aicomp/models/svm/spectrogram_hand_crafted/max_vote_window\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = get_raw_data_dir()\n",
    "\n",
    "processor = EEGDataProcessor(raw_data_path=DATA_PATH, processed_data_path=get_processed_data_dir())\n",
    "train_df = processor.process_data(vote_method=DATA_PREPARATION_VOTE_METHOD, skip_parquet=True)\n",
    "\n",
    "test_df = pd.read_csv(DATA_PATH / \"test.csv\")\n",
    "\n",
    "kl_score = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "\n",
    "models_save_path = get_models_save_path(EXISTING_CHECKPOINT_KAGGLE_DATASET_ID) / \"svm\" / \"spectrogram_hand_crafted\" / DATA_PREPARATION_VOTE_METHOD\n",
    "models_save_path.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Using models save path:\", models_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spectrogram Files into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram_content(spectrogram_file: pathlib.Path):\n",
    "    spectrogram_id = int(spectrogram_file.stem.split(\"_\")[-1])\n",
    "    df = pd.read_parquet(spectrogram_file)\n",
    "    columns = [col for col in df.columns if col != \"time\"]\n",
    "    content = df[columns].values\n",
    "    return spectrogram_id, content, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11138 train spectrogram files to load into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11138/11138 [06:20<00:00, 29.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all train spectrograms into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spectrograms_dir = DATA_PATH / \"train_spectrograms\"\n",
    "spectrogram_files = list(spectrograms_dir.glob(\"*.parquet\"))\n",
    "\n",
    "if not SKIP_TRAIN:\n",
    "  print(f\"Found {len(spectrogram_files)} train spectrogram files to load into memory\")\n",
    "\n",
    "  spectrograms = {}\n",
    "  for file in tqdm(spectrogram_files):\n",
    "    spectrogram_id, content, _ = get_spectrogram_content(file)\n",
    "    spectrograms[spectrogram_id] = content\n",
    "\n",
    "  gc.collect()\n",
    "  print(\"Loaded all train spectrograms into memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Manually extract and engineer the following clinically relevant features from the spectrogram data:\n",
    "1. Regional Band Powers: Average power in each frequency band (delta, theta, alpha, beta) for each brain region (LL, LP, RP, RL), plus total power. Regional variations help seperate generalized and lateralized events. High delta power distinguishes rythmic delta from other patterns.\n",
    "2. Power Rations: Ratios of power between frequency bands (delta-theta, delta-alpha, theta-alpha, low-high). Ratios are often more discriminative than absolute powers because they're less affected by individual differences in skull thickness, electrode impedance, etc.\n",
    "3. Temporal Variability: Standard deviation of each band power over time. High variability can indicate seizure activity. Rythmic patterns often have lower variability.\n",
    "4. Left-right band power differences: Differences in band powers between left and right hemispheres. Lateralized events often show asymmetries.\n",
    "5. Left-right power ratio: Ratio of total power between left and right hemispheres. Again, lateralized events often show asymmetries.\n",
    "\n",
    "In total, this yields 57 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load one spectrogram to get column structure and frequencies\n",
    "sample_file = spectrogram_files[0]\n",
    "_, _, all_columns = get_spectrogram_content(sample_file)\n",
    "\n",
    "# extract frequency information from column names\n",
    "LL_cols = [col for col in all_columns if col.startswith('LL_')]\n",
    "LP_cols = [col for col in all_columns if col.startswith('LP_')]\n",
    "RP_cols = [col for col in all_columns if col.startswith('RP_')]\n",
    "RL_cols = [col for col in all_columns if col.startswith('RL_')]\n",
    "\n",
    "# extract frequencies from column names (they're the same for all regions)\n",
    "freq_array = np.array([float(col.split('_')[1]) for col in LL_cols])\n",
    "\n",
    "# get column indices for each region\n",
    "LL_idx = [all_columns.index(col) for col in LL_cols]\n",
    "LP_idx = [all_columns.index(col) for col in LP_cols]\n",
    "RP_idx = [all_columns.index(col) for col in RP_cols]\n",
    "RL_idx = [all_columns.index(col) for col in RL_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 57\n",
      "['LL_delta_power', 'LL_theta_power', 'LL_alpha_power', 'LL_beta_power', 'LL_total_power', 'LL_delta_theta_ratio', 'LL_delta_alpha_ratio', 'LL_theta_alpha_ratio', 'LL_low_high_ratio', 'LL_delta_std', 'LL_theta_std', 'LL_alpha_std', 'LL_beta_std', 'LP_delta_power', 'LP_theta_power', 'LP_alpha_power', 'LP_beta_power', 'LP_total_power', 'LP_delta_theta_ratio', 'LP_delta_alpha_ratio', 'LP_theta_alpha_ratio', 'LP_low_high_ratio', 'LP_delta_std', 'LP_theta_std', 'LP_alpha_std', 'LP_beta_std', 'RP_delta_power', 'RP_theta_power', 'RP_alpha_power', 'RP_beta_power', 'RP_total_power', 'RP_delta_theta_ratio', 'RP_delta_alpha_ratio', 'RP_theta_alpha_ratio', 'RP_low_high_ratio', 'RP_delta_std', 'RP_theta_std', 'RP_alpha_std', 'RP_beta_std', 'RL_delta_power', 'RL_theta_power', 'RL_alpha_power', 'RL_beta_power', 'RL_total_power', 'RL_delta_theta_ratio', 'RL_delta_alpha_ratio', 'RL_theta_alpha_ratio', 'RL_low_high_ratio', 'RL_delta_std', 'RL_theta_std', 'RL_alpha_std', 'RL_beta_std', 'left_right_delta_diff', 'left_right_theta_diff', 'left_right_alpha_diff', 'left_right_beta_diff', 'left_right_power_ratio']\n"
     ]
    }
   ],
   "source": [
    "FEATURES = []\n",
    "\n",
    "for region in [\"LL\", \"LP\", \"RP\", \"RL\"]:\n",
    "    # band powers\n",
    "    FEATURES.append(f\"{region}_delta_power\")\n",
    "    FEATURES.append(f\"{region}_theta_power\")\n",
    "    FEATURES.append(f\"{region}_alpha_power\")\n",
    "    FEATURES.append(f\"{region}_beta_power\")\n",
    "    FEATURES.append(f\"{region}_total_power\")\n",
    "\n",
    "    # power ratios\n",
    "    FEATURES.append(f\"{region}_delta_theta_ratio\")\n",
    "    FEATURES.append(f\"{region}_delta_alpha_ratio\")\n",
    "    FEATURES.append(f\"{region}_theta_alpha_ratio\")\n",
    "    FEATURES.append(f\"{region}_low_high_ratio\")\n",
    "\n",
    "    # temporal Variability\n",
    "    FEATURES.append(f\"{region}_delta_std\")\n",
    "    FEATURES.append(f\"{region}_theta_std\")\n",
    "    FEATURES.append(f\"{region}_alpha_std\")\n",
    "    FEATURES.append(f\"{region}_beta_std\")\n",
    "\n",
    "# cross-hemispheric Features\n",
    "\n",
    "# absolute differences\n",
    "FEATURES.append(\"left_right_delta_diff\")\n",
    "FEATURES.append(\"left_right_theta_diff\")\n",
    "FEATURES.append(\"left_right_alpha_diff\")\n",
    "FEATURES.append(\"left_right_beta_diff\")\n",
    "\n",
    "# ratios\n",
    "FEATURES.append(\"left_right_power_ratio\")\n",
    "\n",
    "print(f\"Total number of features: {len(FEATURES)}\")\n",
    "print(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clinical_features(ten_minute_window):\n",
    "    features = {}\n",
    "    \n",
    "    # separate regions\n",
    "    LL_data = ten_minute_window[:, LL_idx]\n",
    "    LP_data = ten_minute_window[:, LP_idx]\n",
    "    RP_data = ten_minute_window[:, RP_idx]\n",
    "    RL_data = ten_minute_window[:, RL_idx]\n",
    "    \n",
    "    # define bands\n",
    "    delta_idx = np.where((freq_array >= 0.5) & (freq_array <= 4.0))[0]\n",
    "    theta_idx = np.where((freq_array >= 4.0) & (freq_array < 8.0))[0]\n",
    "    alpha_idx = np.where((freq_array >= 8.0) & (freq_array < 13.0))[0]\n",
    "    beta_idx = np.where((freq_array >= 13.0) & (freq_array < 30.0))[0]\n",
    "    \n",
    "    # band powers\n",
    "    for region_name, region_data in [('LL', LL_data), ('LP', LP_data), ('RP', RP_data), ('RL', RL_data)]:\n",
    "        features[f'{region_name}_delta_power'] = region_data[:, delta_idx].mean()\n",
    "        features[f'{region_name}_theta_power'] = region_data[:, theta_idx].mean()\n",
    "        features[f'{region_name}_alpha_power'] = region_data[:, alpha_idx].mean()\n",
    "        features[f'{region_name}_beta_power'] = region_data[:, beta_idx].mean()\n",
    "        features[f'{region_name}_total_power'] = region_data.mean()\n",
    "    \n",
    "    # power ratios\n",
    "    for region_name, region_data in [('LL', LL_data), ('LP', LP_data), ('RP', RP_data), ('RL', RL_data)]:\n",
    "        features[f'{region_name}_delta_theta_ratio'] = features[f'{region_name}_delta_power'] / (features[f'{region_name}_theta_power'] + 1e-10)\n",
    "        features[f'{region_name}_delta_alpha_ratio'] = features[f'{region_name}_delta_power'] / (features[f'{region_name}_alpha_power'] + 1e-10)\n",
    "        features[f'{region_name}_theta_alpha_ratio'] = features[f'{region_name}_theta_power'] / (features[f'{region_name}_alpha_power'] + 1e-10)\n",
    "        features[f'{region_name}_low_high_ratio'] = (features[f'{region_name}_delta_power'] + features[f'{region_name}_theta_power']) / (features[f'{region_name}_alpha_power'] + features[f'{region_name}_beta_power'] + 1e-10)\n",
    "    \n",
    "    # temporal Variability\n",
    "    for region_name, region_data in [('LL', LL_data), ('LP', LP_data), ('RP', RP_data), ('RL', RL_data)]:\n",
    "        delta_power_over_time = region_data[:, delta_idx].mean(axis=1)\n",
    "        theta_power_over_time = region_data[:, theta_idx].mean(axis=1)\n",
    "        alpha_power_over_time = region_data[:, alpha_idx].mean(axis=1)\n",
    "        beta_power_over_time = region_data[:, beta_idx].mean(axis=1)\n",
    "\n",
    "        features[f'{region_name}_delta_std'] = delta_power_over_time.std()\n",
    "        features[f'{region_name}_theta_std'] = theta_power_over_time.std()\n",
    "        features[f'{region_name}_alpha_std'] = alpha_power_over_time.std()\n",
    "        features[f'{region_name}_beta_std'] = beta_power_over_time.std()\n",
    "\n",
    "    # cross-hemispheric Features\n",
    "\n",
    "    # absolute differences\n",
    "    features['left_right_delta_diff'] = abs(\n",
    "        (features['LL_delta_power'] + features['LP_delta_power']) - \n",
    "        (features['RL_delta_power'] + features['RP_delta_power'])\n",
    "    )\n",
    "\n",
    "    features['left_right_theta_diff'] = abs(\n",
    "        (features['LL_theta_power'] + features['LP_theta_power']) - \n",
    "        (features['RL_theta_power'] + features['RP_theta_power'])\n",
    "    )\n",
    "\n",
    "    features['left_right_alpha_diff'] = abs(\n",
    "        (features['LL_alpha_power'] + features['LP_alpha_power']) - \n",
    "        (features['RL_alpha_power'] + features['RP_alpha_power'])\n",
    "    )\n",
    "\n",
    "    features['left_right_beta_diff'] = abs(\n",
    "        (features['LL_beta_power'] + features['LP_beta_power']) - \n",
    "        (features['RL_beta_power'] + features['RP_beta_power'])\n",
    "    )\n",
    "\n",
    "    # total power ratio\n",
    "    left_total = features['LL_total_power'] + features['LP_total_power']\n",
    "    right_total = features['RL_total_power'] + features['RP_total_power']\n",
    "    features['left_right_power_ratio'] = left_total / (right_total + 1e-10)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectrogram_features(ten_minute_window):\n",
    "  features_dict = extract_clinical_features(ten_minute_window)\n",
    "  return np.array([features_dict[feat_name] for feat_name in FEATURES]) # return features in the same order as FEATURES list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17089/17089 [00:18<00:00, 938.21it/s]\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_TRAIN:\n",
    "  data = np.zeros((len(train_df), len(FEATURES)))\n",
    "\n",
    "  def extract_train_spectrogram_features(row, all_spectrograms):\n",
    "    spectrogram_id = int(row[\"spectrogram_id\"])\n",
    "    middle_offset = (row[\"min_offset\"] + row[\"max_offset\"]) // 2 # this the middle between the least spectrogram offset and greatest spectogram offset\n",
    "    row_index = int(middle_offset // 2) # each spectrogram row corresponds to 2s, so we divide by 2 to get the row index\n",
    "    window = np.array(all_spectrograms[spectrogram_id][row_index:row_index+300,:])\n",
    "    average_frequencies = extract_spectrogram_features(window)\n",
    "    return average_frequencies\n",
    "\n",
    "  for i in tqdm(range(len(train_df)), total=len(train_df)):\n",
    "    row = train_df.iloc[i]\n",
    "    average_features = extract_train_spectrogram_features(row, spectrograms)\n",
    "    data[i,:] = average_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eeg_id  spectrogram_id  patient_id expert_consensus  seizure_vote  \\\n",
      "0  568657       789577333       20654            Other           0.0   \n",
      "1  582999      1552638400       20230              LPD           0.0   \n",
      "2  642382        14960202        5955            Other           0.0   \n",
      "3  751790       618728447       38549              GPD           0.0   \n",
      "4  778705        52296320       40955            Other           0.0   \n",
      "\n",
      "   lpd_vote  gpd_vote  lrda_vote  grda_vote  other_vote  ...  \\\n",
      "0  0.000000      0.25   0.000000   0.166667    0.583333  ...   \n",
      "1  0.857143      0.00   0.071429   0.000000    0.071429  ...   \n",
      "2  0.000000      0.00   0.000000   0.000000    1.000000  ...   \n",
      "3  0.000000      1.00   0.000000   0.000000    0.000000  ...   \n",
      "4  0.000000      0.00   0.000000   0.000000    1.000000  ...   \n",
      "\n",
      "   RL_low_high_ratio  RL_delta_std  RL_theta_std  RL_alpha_std  RL_beta_std  \\\n",
      "0          33.868639   2176.569336    192.093048     60.546051    23.950325   \n",
      "1          23.379087     10.841111      0.619437      0.267831     0.154956   \n",
      "2          22.953094      4.022285      0.405165      0.077095     0.123624   \n",
      "3          34.299041      8.578465      3.910118      0.578711     0.102692   \n",
      "4          79.522775    501.605591      2.030022      0.326083     0.278658   \n",
      "\n",
      "   left_right_delta_diff  left_right_theta_diff  left_right_alpha_diff  \\\n",
      "0             745.254700              62.837337              16.953247   \n",
      "1               0.365795               0.953647               0.189783   \n",
      "2            1599.931519              32.422604               5.628253   \n",
      "3               6.191994               1.871224               0.477472   \n",
      "4           20798.419922            1128.910034             576.095093   \n",
      "\n",
      "   left_right_beta_diff  left_right_power_ratio  \n",
      "0              7.056808                0.395421  \n",
      "1              0.043500                0.956737  \n",
      "2              0.986632                0.007292  \n",
      "3              0.084561                1.035392  \n",
      "4            260.554596                0.002577  \n",
      "\n",
      "[5 rows x 69 columns]\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_TRAIN:\n",
    "  train_df[FEATURES] = data\n",
    "\n",
    "  print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "targets_dict = {\"Seizure\":0, \"LPD\":1, \"GPD\":2, \"LRDA\":3, \"GRDA\":4, \"Other\":5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_TRAIN:\n",
    "    fold_creator = KFoldCreator(n_splits=N_SPLITS, seed=Constants.SEED)\n",
    "    train_folds_df = fold_creator.create_folds(\n",
    "        df=train_df, stratify_col=\"expert_consensus\", group_col=\"patient_id\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "FOLD 0\n",
      "Train size: 13022, Valid size: 4067\n",
      "==============================\n",
      "Training SVM...\n",
      "Predicting on validation set...\n",
      "========================================\n",
      "FOLD 1\n",
      "Train size: 13431, Valid size: 3658\n",
      "==============================\n",
      "Training SVM...\n",
      "Predicting on validation set...\n",
      "========================================\n",
      "FOLD 2\n",
      "Train size: 13708, Valid size: 3381\n",
      "==============================\n",
      "Training SVM...\n",
      "Predicting on validation set...\n",
      "========================================\n",
      "FOLD 3\n",
      "Train size: 14464, Valid size: 2625\n",
      "==============================\n",
      "Training SVM...\n",
      "Predicting on validation set...\n",
      "========================================\n",
      "FOLD 4\n",
      "Train size: 13731, Valid size: 3358\n",
      "==============================\n",
      "Training SVM...\n",
      "Predicting on validation set...\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_TRAIN:\n",
    "    all_oof = []\n",
    "    all_true = []\n",
    "\n",
    "    for fold in range(N_SPLITS):\n",
    "        fold_train_df = train_folds_df[train_folds_df[\"fold\"] != fold].reset_index(drop=True)\n",
    "        fold_valid_df = train_folds_df[train_folds_df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"FOLD {fold}\")\n",
    "        print(f\"Train size: {len(fold_train_df)}, Valid size: {len(fold_valid_df)}\")\n",
    "        print(\"=\" * 30)\n",
    "\n",
    "        X_train = fold_train_df[FEATURES].values\n",
    "        y_train = fold_train_df[\"expert_consensus\"].map(targets_dict).values\n",
    "\n",
    "        X_valid = fold_valid_df[FEATURES].values\n",
    "        y_valid = fold_valid_df[\"expert_consensus\"].map(targets_dict).values\n",
    "\n",
    "        X_train = fill_nan_with_mean(X_train)\n",
    "        X_valid = fill_nan_with_mean(X_valid)\n",
    "\n",
    "        # scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "        # train SVM with probability estimates\n",
    "        model = SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            probability=True,\n",
    "            random_state=Constants.SEED,\n",
    "            verbose=False,\n",
    "            cache_size=1000 # 1 GB\n",
    "        )\n",
    "        \n",
    "        print(\"Training SVM...\")\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        joblib.dump(model, models_save_path / f\"fold_{fold}_model.pkl\")\n",
    "        joblib.dump(scaler, models_save_path / f\"fold_{fold}_scaler.pkl\")\n",
    "        \n",
    "        print(\"Predicting on validation set...\")\n",
    "        oof = model.predict_proba(X_valid_scaled)\n",
    "        all_oof.extend(oof)\n",
    "\n",
    "        all_true.extend(fold_valid_df[Constants.TARGETS].values)\n",
    "\n",
    "        del X_train, y_train, X_valid, y_valid, X_train_scaled, X_valid_scaled, oof\n",
    "        gc.collect()\n",
    "\n",
    "    all_oof = np.array(all_oof)\n",
    "    all_true = np.array(all_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF KL Score: 1.1904058456420898\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_TRAIN:\n",
    "  all_oof_tensor = torch.tensor(all_oof, dtype=torch.float32)\n",
    "  all_true_tensor = torch.tensor(all_true, dtype=torch.float32)\n",
    "\n",
    "  kl_score = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "  score = kl_score(all_oof_tensor.log(), all_true_tensor).item()\n",
    "\n",
    "  print(f\"OOF KL Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer on Test and create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 test spectrogram files to load into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 28.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all test spectrograms into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_spectrograms_dir = DATA_PATH / \"test_spectrograms\"\n",
    "test_spectrogram_files = list(test_spectrograms_dir.glob(\"*.parquet\"))\n",
    "print(f\"Found {len(test_spectrogram_files)} test spectrogram files to load into memory\")\n",
    "\n",
    "test_spectrograms = {}\n",
    "for file in tqdm(test_spectrogram_files):\n",
    "  spectrogram_id, content, _ = get_spectrogram_content(file)\n",
    "  test_spectrograms[spectrogram_id] = content\n",
    "\n",
    "gc.collect()\n",
    "print(\"Loaded all test spectrograms into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 543.94it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = np.zeros((len(test_df), len(FEATURES)))\n",
    "\n",
    "def extract_test_spectrogram_features(row, all_spectrograms):\n",
    "  # this differs from train because all test spectrograms are exactly 10 minutes long, so we don't need to extract the center window\n",
    "  spectrogram_id = int(row[\"spectrogram_id\"])\n",
    "  content = np.array(all_spectrograms[spectrogram_id][:])\n",
    "  average_frequencies = extract_spectrogram_features(content)\n",
    "  return average_frequencies\n",
    "\n",
    "for i in tqdm(range(len(test_df)), total=len(test_df)):\n",
    "  row = test_df.iloc[i]\n",
    "  average_features = extract_test_spectrogram_features(row, test_spectrograms)\n",
    "  test_data[i,:] = average_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>LL_delta_power</th>\n",
       "      <th>LL_theta_power</th>\n",
       "      <th>LL_alpha_power</th>\n",
       "      <th>LL_beta_power</th>\n",
       "      <th>LL_total_power</th>\n",
       "      <th>LL_delta_theta_ratio</th>\n",
       "      <th>LL_delta_alpha_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>RL_low_high_ratio</th>\n",
       "      <th>RL_delta_std</th>\n",
       "      <th>RL_theta_std</th>\n",
       "      <th>RL_alpha_std</th>\n",
       "      <th>RL_beta_std</th>\n",
       "      <th>left_right_delta_diff</th>\n",
       "      <th>left_right_theta_diff</th>\n",
       "      <th>left_right_alpha_diff</th>\n",
       "      <th>left_right_beta_diff</th>\n",
       "      <th>left_right_power_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>853520</td>\n",
       "      <td>3911565283</td>\n",
       "      <td>6885</td>\n",
       "      <td>5.342817</td>\n",
       "      <td>0.455045</td>\n",
       "      <td>0.621987</td>\n",
       "      <td>1.349531</td>\n",
       "      <td>1.700264</td>\n",
       "      <td>11.741293</td>\n",
       "      <td>8.589915</td>\n",
       "      <td>...</td>\n",
       "      <td>79.804853</td>\n",
       "      <td>32.726185</td>\n",
       "      <td>0.364277</td>\n",
       "      <td>0.092094</td>\n",
       "      <td>0.072847</td>\n",
       "      <td>23.825184</td>\n",
       "      <td>0.79198</td>\n",
       "      <td>0.379858</td>\n",
       "      <td>1.352977</td>\n",
       "      <td>0.536792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   spectrogram_id      eeg_id  patient_id  LL_delta_power  LL_theta_power  \\\n",
       "0          853520  3911565283        6885        5.342817        0.455045   \n",
       "\n",
       "   LL_alpha_power  LL_beta_power  LL_total_power  LL_delta_theta_ratio  \\\n",
       "0        0.621987       1.349531        1.700264             11.741293   \n",
       "\n",
       "   LL_delta_alpha_ratio  ...  RL_low_high_ratio  RL_delta_std  RL_theta_std  \\\n",
       "0              8.589915  ...          79.804853     32.726185      0.364277   \n",
       "\n",
       "   RL_alpha_std  RL_beta_std  left_right_delta_diff  left_right_theta_diff  \\\n",
       "0      0.092094     0.072847              23.825184                0.79198   \n",
       "\n",
       "   left_right_alpha_diff  left_right_beta_diff  left_right_power_ratio  \n",
       "0               0.379858              1.352977                0.536792  \n",
       "\n",
       "[1 rows x 60 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[FEATURES] = test_data\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Predicting fold 0\n",
      "========================================\n",
      "========================================\n",
      "Predicting fold 1\n",
      "========================================\n",
      "========================================\n",
      "Predicting fold 2\n",
      "========================================\n",
      "========================================\n",
      "Predicting fold 3\n",
      "========================================\n",
      "========================================\n",
      "Predicting fold 4\n",
      "========================================\n",
      "Test predictions shape: (1, 6)\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "  print(\"=\" * 40)\n",
    "  print(f\"Predicting fold {fold}\")\n",
    "  print(\"=\" * 40)\n",
    "\n",
    "  X_test = test_df[FEATURES].values\n",
    "  X_test = fill_nan_with_mean(X_test)\n",
    "\n",
    "  scaler = joblib.load(models_save_path / f\"fold_{fold}_scaler.pkl\")\n",
    "  model = joblib.load(models_save_path / f\"fold_{fold}_model.pkl\")\n",
    "\n",
    "  X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "  preds = model.predict_proba(X_test_scaled)\n",
    "  test_preds.append(preds)\n",
    "\n",
    "test_preds = np.mean(test_preds, axis=0)\n",
    "print(f\"Test predictions shape: {test_preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: all predictions should sum to 1\n",
    "assert np.allclose(test_preds.sum(axis=1), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"eeg_id\": test_df[\"eeg_id\"]})\n",
    "submission[Constants.TARGETS] = test_preds\n",
    "\n",
    "submission.to_csv(get_submission_csv_path(), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
